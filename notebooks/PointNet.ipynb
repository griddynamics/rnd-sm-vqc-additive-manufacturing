{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reqruirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctOnzDAlA56B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import open3d as o3d\n",
        "from joblib import load, dump\n",
        "\n",
        "import torch\n",
        "from scipy.special import softmax\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "random.seed = 42"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m931bqhmPE9L"
      },
      "source": [
        "### MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vdoy-SqUO--t"
      },
      "outputs": [],
      "source": [
        "# Used for extracting features giving a 1 dimensional vector for point net\n",
        "\n",
        "class Tnet(nn.Module):\n",
        "   def __init__(self, k=3):\n",
        "      super().__init__()\n",
        "      self.k=k\n",
        "      self.conv1 = nn.Conv1d(k,64,1)\n",
        "      self.conv2 = nn.Conv1d(64,128,1)\n",
        "      self.conv3 = nn.Conv1d(128,1024,1)\n",
        "      self.fc1 = nn.Linear(1024,512)\n",
        "      self.fc2 = nn.Linear(512,256)\n",
        "      self.fc3 = nn.Linear(256,k*k)\n",
        "\n",
        "      self.bn1 = nn.BatchNorm1d(64)\n",
        "      self.bn2 = nn.BatchNorm1d(128)\n",
        "      self.bn3 = nn.BatchNorm1d(1024)\n",
        "      self.bn4 = nn.BatchNorm1d(512)\n",
        "      self.bn5 = nn.BatchNorm1d(256)\n",
        "       \n",
        "\n",
        "   def forward(self, input):\n",
        "      # input.shape == (bs,n,3)\n",
        "      bs = input.size(0)\n",
        "      xb = F.relu(self.bn1(self.conv1(input)))\n",
        "      xb = F.relu(self.bn2(self.conv2(xb)))\n",
        "      xb = F.relu(self.bn3(self.conv3(xb)))\n",
        "      pool = nn.MaxPool1d(xb.size(-1))(xb)\n",
        "      flat = nn.Flatten(1)(pool)\n",
        "      xb = F.relu(self.bn4(self.fc1(flat)))\n",
        "      xb = F.relu(self.bn5(self.fc2(xb)))\n",
        "      \n",
        "      #initialize as identity\n",
        "      init = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)\n",
        "      if xb.is_cuda:\n",
        "        init=init.cuda()\n",
        "      matrix = self.fc3(xb).view(-1,self.k,self.k) + init\n",
        "      return matrix\n",
        "\n",
        "# Used for position estimation and point estimation using global and\n",
        "#  local coordinates\n",
        "class Transform(nn.Module):\n",
        "   def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input_transform = Tnet(k=3)\n",
        "        self.feature_transform = Tnet(k=64)\n",
        "        self.conv1 = nn.Conv1d(3,64,1)#(3,64,1)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(64,128,1)\n",
        "        self.conv3 = nn.Conv1d(128,1024,1)\n",
        "       \n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "       \n",
        "   def forward(self, input):\n",
        "        matrix3x3 = self.input_transform(input)\n",
        "        # batch matrix multiplication\n",
        "        xb = torch.bmm(torch.transpose(input,1,2), matrix3x3).transpose(1,2)\n",
        "\n",
        "        xb = F.relu(self.bn1(self.conv1(xb)))\n",
        "\n",
        "        matrix64x64 = self.feature_transform(xb)\n",
        "        xb = torch.bmm(torch.transpose(xb,1,2), matrix64x64).transpose(1,2)\n",
        "\n",
        "        xb = F.relu(self.bn2(self.conv2(xb)))\n",
        "        xb = self.bn3(self.conv3(xb))\n",
        "        xb = nn.MaxPool1d(xb.size(-1))(xb)\n",
        "        output = nn.Flatten(1)(xb)\n",
        "        return output, matrix3x3, matrix64x64\n",
        "\n",
        "# Classifier\n",
        "class PointNet(nn.Module):\n",
        "    def __init__(self, classes = 2):\n",
        "        super().__init__()\n",
        "        self.transform = Transform()\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, classes)\n",
        "        \n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "      # input of size (batch_size, 3, sample_rate)\n",
        "        xb, matrix3x3, matrix64x64 = self.transform(input) # Returns (batch_size, 1024) dimensional vector\n",
        "        xb = F.relu(self.bn1(self.fc1(xb)))\n",
        "        xb = F.relu(self.bn2(self.dropout(self.fc2(xb))))\n",
        "        output = self.fc3(xb)\n",
        "        return self.logsoftmax(output), matrix3x3, matrix64x64"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PointNet loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQOJneuBPy_C"
      },
      "outputs": [],
      "source": [
        "# PointNet loss calculating with first 3 model outputs\n",
        "\n",
        "# LOSS\n",
        "def pointnetloss(outputs, labels, m3x3, m64x64, alpha = 0.0001):\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "    bs = outputs.size(0)\n",
        "    id3x3 = torch.eye(3, requires_grad=True).repeat(bs, 1, 1)\n",
        "    id64x64 = torch.eye(64, requires_grad=True).repeat(bs, 1, 1)\n",
        "    if outputs.is_cuda:\n",
        "        id3x3 = id3x3.cuda()\n",
        "        id64x64 = id64x64.cuda()\n",
        "    diff3x3 = id3x3 - torch.bmm(m3x3, m3x3.transpose(1, 2))\n",
        "    diff64x64 = id64x64 - torch.bmm(m64x64, m64x64.transpose(1, 2))\n",
        "    return criterion(outputs, labels) + alpha * (torch.norm(diff3x3) + torch.norm(diff64x64)) / float(bs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lD75D6nLRV9w"
      },
      "outputs": [],
      "source": [
        "# Normalization used as a preprocessing step\n",
        "class Normalize(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        assert len(pointcloud.shape)==2\n",
        "        \n",
        "        norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0) \n",
        "        norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))\n",
        "\n",
        "        return  norm_pointcloud\n",
        "\n",
        "def default_transforms():\n",
        "    return transforms.Compose([\n",
        "                                Normalize(),\n",
        "                                \n",
        "                              ])\n",
        "\n",
        "class PointCloudData(Dataset):\n",
        "    def __init__(self, dataframe_path, valid=False, sample_rate=1024, transform=default_transforms()):\n",
        "        # Get data\n",
        "        self.df = pd.read_csv(dataframe_path)\n",
        "        # class dict\n",
        "        self.classes = {\"anomaly\": 1, \"normal\": 0}\n",
        "        self.sample_rate=sample_rate\n",
        "        self.transforms=transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __preproc__(self, file):\n",
        "      # Cloud is loaded\n",
        "        point_cloud = o3d.io.read_point_cloud(file)\n",
        "        # Cloud is reduced to fixed size for ingestion into model\n",
        "        resampled = point_cloud.farthest_point_down_sample(num_samples=self.sample_rate)\n",
        "        np_array = np.array(resampled.points)\n",
        "        if self.transforms:\n",
        "            pointcloud = self.transforms(np_array)\n",
        "        return torch.from_numpy(pointcloud)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pcd_path = os.path.abspath(self.df.iloc[idx]['object_path'])\n",
        "        category = self.df.iloc[idx]['label']\n",
        "        pointcloud = self.__preproc__(pcd_path)\n",
        "        return {'pointcloud': pointcloud, \n",
        "                'category': self.classes[category]}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opg-Z13sA3Zq"
      },
      "outputs": [],
      "source": [
        "# Load datasets generated in Random Forest notebook\n",
        "train_df = pd.read_csv(\"./data/train_df.csv\")\n",
        "test_df = pd.read_csv(\"./data/test_df.csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tHoIJSxpiMnO"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "sample_rate = 1024\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2-NUNGPiNsF",
        "outputId": "616d7bf9-3446-47fd-ab44-79e65e66d5c2"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "pointnet = PointNet()\n",
        "pointnet.to(device)\n",
        "optimizer = torch.optim.Adam(pointnet.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "train_ds = PointCloudData(\"./data/train_df.csv\", sample_rate=sample_rate)\n",
        "train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### We use the below function to train both versions of PointNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZtp_WcsU55q"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, learning_rate, epochs=5, model_name=\"\"):\n",
        "      for epoch in range(epochs): \n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        batch_loss = []\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            pcd = data['pointcloud'].to(device).float()\n",
        "            labels = data['category'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs, m3x3, m64x64 = model(pcd.transpose(1,2))\n",
        "            loss = pointnetloss(outputs, labels, m3x3, m64x64, alpha=learning_rate)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:    # print every 10 mini-batches\n",
        "                    print('[Epoch: %d, Batch: %4d / %4d], loss: %.3f' %\n",
        "                        (epoch + 1, i + 1, len(train_loader), running_loss / 10))\n",
        "                    running_loss = 0.0\n",
        "\n",
        "        torch.save(model.state_dict(), f\"./model_weights/{model_name}_\"+str(epoch)+\".pth\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72pNhijIVRKq"
      },
      "outputs": [],
      "source": [
        "train(pointnet, train_loader, learning_rate, save=True, epochs=epochs, model_name=\"PointNet\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zWoMMCnfVQUd"
      },
      "source": [
        "### Model evaluation\n",
        "\n",
        "Here we evaluate the model for each epoch to select best results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUEfhZ1QX5HZ"
      },
      "outputs": [],
      "source": [
        "test_ds = PointCloudData(\"./data/test_df.csv\", valid=True)\n",
        "test_loader = DataLoader(dataset=test_ds, batch_size=batch_size)\n",
        "for i in range(epochs):\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(device)\n",
        "  print(f\"EPOCH: {i}\")\n",
        "\n",
        "  pointnet = PointNet()\n",
        "  pointnet.to(device)\n",
        "  optimizer = torch.optim.Adam(pointnet.parameters(), lr=learning_rate)\n",
        "\n",
        "  pointnet.load_state_dict(torch.load(f\"./model_weights/PointNet_\"+str(i)+\".pth\"))\n",
        "  pointnet.eval();\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "  with torch.no_grad():\n",
        "      for i, data in enumerate(test_loader):\n",
        "          print('Batch [%4d / %4d]' % (i+1, len(test_loader)))\n",
        "                    \n",
        "          pcd = data['pointcloud'].to(device).float()\n",
        "          labels = data['category'].to(device)\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs, m3x3, m64x64 = pointnet(pcd.transpose(1,2))\n",
        "          _, preds = torch.max(outputs.data, 1)\n",
        "          all_preds += list(preds.cpu().numpy())\n",
        "          all_labels += list(labels.cpu().numpy())\n",
        "  cm = confusion_matrix(all_labels, all_preds);\n",
        "  print(cm)\n",
        "  print(\"*\"*40)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 90/10 Class split evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83ZjWa-Tz7nr",
        "outputId": "d17ea7e8-676f-49d7-ec7a-b27787f9c4cb"
      },
      "outputs": [],
      "source": [
        "# Loading 90/10 split created in random forest noteobok\n",
        "test_ds_90_10 = PointCloudData(\"./data/test_df_90_10.csv\", valid=True)\n",
        "test_loader_90_10 = DataLoader(dataset=test_ds_90_10, batch_size=batch_size)\n",
        "for i in range(epochs):\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(device)\n",
        "  print(f\"EPOCH: {i}\")\n",
        "\n",
        "  pointnet = PointNet()\n",
        "  pointnet.to(device)\n",
        "  optimizer = torch.optim.Adam(pointnet.parameters(), lr=learning_rate)\n",
        "\n",
        "  pointnet.load_state_dict(torch.load(f\"./model_weights/PointNet_\"+str(i)+\".pth\"))\n",
        "  pointnet.eval();\n",
        "  all_preds = []\n",
        "\n",
        "\n",
        "  all_preds_90_10 = []\n",
        "  all_labels_90_10 = []\n",
        "  with torch.no_grad():\n",
        "      for i, data in enumerate(test_loader_90_10):\n",
        "          print('Batch [%4d / %4d]' % (i+1, len(test_loader_90_10)))\n",
        "                    \n",
        "          pcd = data['pointcloud'].to(device).float()\n",
        "          labels = data['category'].to(device)\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs, m3x3, m64x64 = pointnet(pcd.transpose(1,2))\n",
        "          _, preds = torch.max(outputs.data, 1)\n",
        "          all_preds_90_10 += list(preds.cpu().numpy())\n",
        "          all_labels_90_10 += list(labels.cpu().numpy())\n",
        "\n",
        "  cm_90_10 = confusion_matrix(all_labels_90_10, all_preds_90_10);\n",
        "  print(cm_90_10)\n",
        "  print(\"*\"*40)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr0ERntMYp4o"
      },
      "source": [
        "# POINT NET + RandomForestSampling\n",
        "\n",
        "Here we use the same model with a different data loader to train on Random Forest created samples instead of\n",
        "Iterative Farthest point samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Lt_aIeaGkaI"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "sample_rate = 1024\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ssGyWq4gUp_"
      },
      "outputs": [],
      "source": [
        "def create_rf_point_samples(df, rf_point, sample_rate, name):\n",
        "  '''\n",
        "  Function used for creating data using random forest predictions\n",
        "  per point as weights.\n",
        "  Params:\n",
        "  df (pd.DataFrame)\n",
        "  rf_point - Pretrained Random Forest propensity model\n",
        "  sample_rate (int) - Number of samples to select\n",
        "  name (str) - train/test folders\n",
        "\n",
        "  Returns:\n",
        "  pd.DataFrame\n",
        "  '''\n",
        "  folder = \"./data/rf_point_samples_\"+name\n",
        "  os.makedirs(folder, exist_ok=True)\n",
        "  new_path_list = []\n",
        "  for idx, path in enumerate(df[\"reference_path\"]):\n",
        "    tmp_df = pd.read_csv(path)\n",
        "    # \n",
        "    preds = rf_point.predict_proba(tmp_df.loc[:, [\"x\", \"y\", \"z\", \"dist\"]].values)[:, 1]\n",
        "    sampled_inds = np.random.choice(range(tmp_df.shape[0]), size=sample_rate, p=softmax(preds), replace=False)\n",
        "    resampled_df = tmp_df.iloc[sampled_inds]\n",
        "    name = pathlib.Path(path).name\n",
        "    new_path = folder+\"/\"+ str(idx) + \".csv\"\n",
        "    resampled_df.loc[:, ['x', 'y', 'z']].to_csv(new_path, index=False)\n",
        "  \n",
        "    new_path_list.append(new_path)\n",
        "  df[\"sampled_pcds\"] = new_path_list\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0o_lQjX31tW"
      },
      "outputs": [],
      "source": [
        "# Cell needs to be run to create Random Forest samples to be used in PointNet(RF)+RandomForest notebook\n",
        "# Load trained RF model for downsample point selection\n",
        "rf_point = load(\"./model_weights/rf_point.joblib\")\n",
        "\n",
        "RF_sampled_df = create_rf_point_samples(train_df, rf_point, sample_rate, \"train\")\n",
        "RF_sampled_df.to_csv(\"./data/RF_sampled_df.csv\", index=False)\n",
        "\n",
        "\n",
        "RF_sampled_df_test = create_rf_point_samples(test_df, rf_point, sample_rate, \"test\")\n",
        "RF_sampled_df_test.to_csv(\"./data/RF_sampled_df_test.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QW2YNH3axIo"
      },
      "outputs": [],
      "source": [
        "class PointCloudDataRFSampler(Dataset):\n",
        "    def __init__(self, dataframe_path, valid=False, sample_rate=1024, transform=default_transforms()):\n",
        "        self.df = pd.read_csv(dataframe_path)\n",
        "        self.classes = {\"anomaly\": 1, \"normal\": 0}\n",
        "        self.sample_rate=sample_rate\n",
        "        self.transforms=transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __preproc__(self, file):\n",
        "        # Loads point cloud\n",
        "        resampled = pd.read_csv(file).values\n",
        "        # Apply transforms (only normalize but additional can be added)\n",
        "        if self.transforms:\n",
        "            pointcloud = self.transforms(resampled)\n",
        "        return torch.from_numpy(pointcloud)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pcd_path = self.df.iloc[idx]['sampled_pcds']\n",
        "        category = self.df.iloc[idx]['label']\n",
        "        pointcloud = self.__preproc__(pcd_path)\n",
        "        return {'pointcloud': pointcloud, \n",
        "                'category': self.classes[category]}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOdMzsc_M2pp",
        "outputId": "60dc326e-f576-47c2-83e4-c9853e4c6c33"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "pointnet = PointNet()\n",
        "pointnet.to(device)\n",
        "optimizer = torch.optim.Adam(pointnet.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZDrAKNoM3Ru"
      },
      "outputs": [],
      "source": [
        "train_point_ds = PointCloudDataRFSampler(\"./data/RF_sampled_df.csv\", sample_rate=sample_rate)\n",
        "train_loader = DataLoader(train_point_ds, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMTD7wqEM3fu",
        "outputId": "745f4acb-92dc-4674-e7bd-3340ddf0450e"
      },
      "outputs": [],
      "source": [
        "print(len(train_point_ds))\n",
        "train(pointnet, train_loader, learning_rate, save=True, epochs=epochs, model_name=\"PointNet_RF_samples\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "p3G55ILAsyyz"
      },
      "source": [
        "### EVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pbjoh3EgwhEn"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "sample_rate = 1024\n",
        "epochs = 10\n",
        "\n",
        "test_point_ds = PointCloudDataRFSampler(\"./data/RF_sampled_df_test.csv\", sample_rate=sample_rate)\n",
        "test_loader = DataLoader(test_point_ds, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz-I6cosw4E9",
        "outputId": "6773863c-b30b-4b27-fcd3-c325d4601118"
      },
      "outputs": [],
      "source": [
        "\n",
        "for i in range(0,10):\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(device)\n",
        "\n",
        "  pointnet = PointNet()\n",
        "  pointnet.to(device)\n",
        "  optimizer = torch.optim.Adam(pointnet.parameters(), lr=learning_rate)\n",
        "\n",
        "  pointnet.load_state_dict(torch.load(f\"./model_weights//PointNet_{i}.pth\"))\n",
        "\n",
        "  pointnet.eval();\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "  print(f\"EPOCH: {i}\")\n",
        "  with torch.no_grad():\n",
        "      for i, data in enumerate(test_loader):\n",
        "          print('Batch [%4d / %4d]' % (i+1, len(test_loader)))\n",
        "                    \n",
        "          pcd = data['pointcloud'].to(device).float()\n",
        "          labels = data['category'].to(device)\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs, m3x3, m64x64 = pointnet(pcd.transpose(1,2))\n",
        "          _, preds = torch.max(outputs.data, 1)\n",
        "          all_preds += list(preds.cpu().numpy())\n",
        "          all_labels += list(labels.cpu().numpy())\n",
        "  cm = confusion_matrix(all_labels, all_preds);\n",
        "  print(cm)\n",
        "  print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7yUQssB0Uif",
        "outputId": "704cf71f-607d-444e-fc58-6a8d6e3c3dc9"
      },
      "outputs": [],
      "source": [
        "test_90_10 = pd.read_csv(\"./data/test_df_90_10.csv\")\n",
        "test_df = pd.read_csv(\"./data/RF_sampled_df_test.csv\")\n",
        "inds = []\n",
        "for row in test_df.iterrows():\n",
        "  if row[1][\"object_path\"] in test_90_10[\"object_path\"].tolist():\n",
        "    inds.append(row[0])\n",
        "\n",
        "len(inds)\n",
        "# Match indices of 90/10 dataset with 50/50 full dataset of RF sampled points\n",
        "# Save train dataset\n",
        "test_df.iloc[inds].to_csv(\"./data/RF_sampled_df_test_90_10.csv\", index=False)\n",
        "\n",
        "test_point_ds_90_10 = PointCloudDataRFSampler(\"./data/RF_sampled_df_test_90_10.csv\", sample_rate=sample_rate)\n",
        "test_loader_90_10 = DataLoader(test_point_ds_90_10, batch_size=batch_size)\n",
        "\n",
        "for i in range(0,10):\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(device)\n",
        "\n",
        "  pointnet = PointNet()\n",
        "  pointnet.to(device)\n",
        "  optimizer = torch.optim.Adam(pointnet.parameters(), lr=learning_rate)\n",
        "\n",
        "  pointnet.load_state_dict(torch.load(f\"/content/PointNet_RF_samples_{i}.pth\"))\n",
        "\n",
        "  pointnet.eval();\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "  print(f\"EPOCH: {i}\")\n",
        "  with torch.no_grad():\n",
        "      for i, data in enumerate(test_loader_90_10):\n",
        "          print('Batch [%4d / %4d]' % (i+1, len(test_loader_90_10)))\n",
        "                    \n",
        "          pcd = data['pointcloud'].to(device).float()\n",
        "          labels = data['category'].to(device)\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs, m3x3, m64x64 = pointnet(pcd.transpose(1,2))\n",
        "          _, preds = torch.max(outputs.data, 1)\n",
        "          all_preds += list(preds.cpu().numpy())\n",
        "          all_labels += list(labels.cpu().numpy())\n",
        "  cm = confusion_matrix(all_labels, all_preds);\n",
        "  print(cm)\n",
        "  print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
