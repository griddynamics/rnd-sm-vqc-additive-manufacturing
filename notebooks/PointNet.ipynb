{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEZ-7yeD_pvF",
        "outputId": "7d7f40ec-dbc2-422a-ccc4-90f946f87c5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "# # Mount drive with datasets\n",
        "# drive.mount('/content/drive')\n",
        "# # Copy files to notebook\n",
        "# !cp \"/content/drive/MyDrive/ma-vqc_datasets/Quartered anomaly size.zip\" \"/content\"\n",
        "\n",
        "# # Install open3d (make sure to restart the runtime for the installation to be complete)\n",
        "# !pip install open3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\sani\\anaconda3\\envs\\ma-vqc-data-preprocessing\\lib\\site-packages (2.0.1)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.15.2-cp310-cp310-win_amd64.whl (1.2 MB)\n",
            "     ---------------------------------------- 1.2/1.2 MB 3.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: networkx in c:\\users\\sani\\anaconda3\\envs\\ma-vqc-data-preprocessing\\lib\\site-packages (from torch) (3.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\sani\\anaconda3\\envs\\ma-vqc-data-preprocessing\\lib\\site-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\sani\\anaconda3\\envs\\ma-vqc-data-preprocessing\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\sani\\anaconda3\\envs\\ma-vqc-data-preprocessing\\lib\\site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\sani\\anaconda3\\envs\\ma-vqc-data-preprocessing\\lib\\site-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\sani\\anaconda3\\envs\\ma-vqc-data-preprocessing\\lib\\site-packages (from torchvision) (1.24.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sani\\anaconda3\\envs\\ma-vqc-data-preprocessing\\lib\\site-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: requests in c:\\users\\sani\\anaconda3\\envs\\ma-vqc-data-preprocessing\\lib\\site-packages (from torchvision) (2.28.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sani\\anaconda3\\envs\\ma-vqc-data-preprocessing\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sani\\anaconda3\\envs\\ma-vqc-data-preprocessing\\lib\\site-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sani\\anaconda3\\envs\\ma-vqc-data-preprocessing\\lib\\site-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sani\\anaconda3\\envs\\ma-vqc-data-preprocessing\\lib\\site-packages (from requests->torchvision) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sani\\anaconda3\\envs\\ma-vqc-data-preprocessing\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sani\\anaconda3\\envs\\ma-vqc-data-preprocessing\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.15.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ctOnzDAlA56B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import open3d as o3d\n",
        "import zipfile\n",
        "import glob\n",
        "from joblib import load, dump\n",
        "import pathlib\n",
        "\n",
        "from scipy.special import softmax\n",
        "import torch\n",
        "import scipy.spatial.distance\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torch import nn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "random.seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIfQ8xxF8jCC"
      },
      "outputs": [],
      "source": [
        "# # Extract dataset\n",
        "# zip_name = \"Quartered anomaly size\"\n",
        "\n",
        "# with zipfile.ZipFile(f\"/content/{zip_name}.zip\", 'r') as zip_ref: \n",
        "#     zip_ref.extractall(\"/content/\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m931bqhmPE9L"
      },
      "source": [
        "### MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Vdoy-SqUO--t"
      },
      "outputs": [],
      "source": [
        "# Used for extracting features giving a 1 dimensional vector for point net\n",
        "\n",
        "class Tnet(nn.Module):\n",
        "   def __init__(self, k=3):\n",
        "      super().__init__()\n",
        "      self.k=k\n",
        "      self.conv1 = nn.Conv1d(k,64,1)\n",
        "      self.conv2 = nn.Conv1d(64,128,1)\n",
        "      self.conv3 = nn.Conv1d(128,1024,1)\n",
        "      self.fc1 = nn.Linear(1024,512)\n",
        "      self.fc2 = nn.Linear(512,256)\n",
        "      self.fc3 = nn.Linear(256,k*k)\n",
        "\n",
        "      self.bn1 = nn.BatchNorm1d(64)\n",
        "      self.bn2 = nn.BatchNorm1d(128)\n",
        "      self.bn3 = nn.BatchNorm1d(1024)\n",
        "      self.bn4 = nn.BatchNorm1d(512)\n",
        "      self.bn5 = nn.BatchNorm1d(256)\n",
        "       \n",
        "\n",
        "   def forward(self, input):\n",
        "      # input.shape == (bs,n,3)\n",
        "      bs = input.size(0)\n",
        "      xb = F.relu(self.bn1(self.conv1(input)))\n",
        "      xb = F.relu(self.bn2(self.conv2(xb)))\n",
        "      xb = F.relu(self.bn3(self.conv3(xb)))\n",
        "      pool = nn.MaxPool1d(xb.size(-1))(xb)\n",
        "      flat = nn.Flatten(1)(pool)\n",
        "      xb = F.relu(self.bn4(self.fc1(flat)))\n",
        "      xb = F.relu(self.bn5(self.fc2(xb)))\n",
        "      \n",
        "      #initialize as identity\n",
        "      init = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)\n",
        "      if xb.is_cuda:\n",
        "        init=init.cuda()\n",
        "      matrix = self.fc3(xb).view(-1,self.k,self.k) + init\n",
        "      return matrix\n",
        "\n",
        "# Used for position estimation and point estimation using global and\n",
        "#  local coordinates\n",
        "class Transform(nn.Module):\n",
        "   def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input_transform = Tnet(k=3)\n",
        "        self.feature_transform = Tnet(k=64)\n",
        "        self.conv1 = nn.Conv1d(3,64,1)#(3,64,1)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(64,128,1)\n",
        "        self.conv3 = nn.Conv1d(128,1024,1)\n",
        "       \n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "       \n",
        "   def forward(self, input):\n",
        "        matrix3x3 = self.input_transform(input)\n",
        "        # batch matrix multiplication\n",
        "        xb = torch.bmm(torch.transpose(input,1,2), matrix3x3).transpose(1,2)\n",
        "\n",
        "        xb = F.relu(self.bn1(self.conv1(xb)))\n",
        "\n",
        "        matrix64x64 = self.feature_transform(xb)\n",
        "        xb = torch.bmm(torch.transpose(xb,1,2), matrix64x64).transpose(1,2)\n",
        "\n",
        "        xb = F.relu(self.bn2(self.conv2(xb)))\n",
        "        xb = self.bn3(self.conv3(xb))\n",
        "        xb = nn.MaxPool1d(xb.size(-1))(xb)\n",
        "        output = nn.Flatten(1)(xb)\n",
        "        return output, matrix3x3, matrix64x64\n",
        "\n",
        "# Classifier\n",
        "class PointNet(nn.Module):\n",
        "    def __init__(self, classes = 2):\n",
        "        super().__init__()\n",
        "        self.transform = Transform()\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, classes)\n",
        "        \n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "      # input of size (batch_size, 3, sample_rate)\n",
        "        xb, matrix3x3, matrix64x64 = self.transform(input) # Returns (batch_size, 1024) dimensional vector\n",
        "        xb = F.relu(self.bn1(self.fc1(xb)))\n",
        "        xb = F.relu(self.bn2(self.dropout(self.fc2(xb))))\n",
        "        output = self.fc3(xb)\n",
        "        return self.logsoftmax(output), matrix3x3, matrix64x64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VQOJneuBPy_C"
      },
      "outputs": [],
      "source": [
        "# PointNet loss calculating with first 3 model outputs\n",
        "\n",
        "# LOSS\n",
        "def pointnetloss(outputs, labels, m3x3, m64x64, alpha = 0.0001):\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "    bs = outputs.size(0)\n",
        "    id3x3 = torch.eye(3, requires_grad=True).repeat(bs, 1, 1)\n",
        "    id64x64 = torch.eye(64, requires_grad=True).repeat(bs, 1, 1)\n",
        "    if outputs.is_cuda:\n",
        "        id3x3 = id3x3.cuda()\n",
        "        id64x64 = id64x64.cuda()\n",
        "    diff3x3 = id3x3 - torch.bmm(m3x3, m3x3.transpose(1, 2))\n",
        "    diff64x64 = id64x64 - torch.bmm(m64x64, m64x64.transpose(1, 2))\n",
        "    return criterion(outputs, labels) + alpha * (torch.norm(diff3x3) + torch.norm(diff64x64)) / float(bs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lD75D6nLRV9w"
      },
      "outputs": [],
      "source": [
        "# Normalization used as a preprocessing step\n",
        "class Normalize(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        assert len(pointcloud.shape)==2\n",
        "        \n",
        "        norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0) \n",
        "        norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))\n",
        "\n",
        "        return  norm_pointcloud\n",
        "\n",
        "def default_transforms():\n",
        "    return transforms.Compose([\n",
        "                                Normalize(),\n",
        "                                \n",
        "                              ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "je-9LmU6Pcat"
      },
      "outputs": [],
      "source": [
        "class PointCloudData(Dataset):\n",
        "    def __init__(self, dataframe_path, valid=False, sample_rate=1024, transform=default_transforms()):\n",
        "        # Get data\n",
        "        self.df = pd.read_csv(dataframe_path)\n",
        "        # class dict\n",
        "        self.classes = {\"anomaly\": 1, \"normal\": 0}\n",
        "        self.sample_rate=sample_rate\n",
        "        self.transforms=transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __preproc__(self, file):\n",
        "      # Cloud is loaded\n",
        "        point_cloud = o3d.io.read_point_cloud(file)\n",
        "        # Cloud is reduced to fixed size for ingestion into model\n",
        "        resampled = point_cloud.farthest_point_down_sample(num_samples=self.sample_rate)\n",
        "        np_array = np.array(resampled.points)\n",
        "        if self.transforms:\n",
        "            pointcloud = self.transforms(np_array)\n",
        "        return torch.from_numpy(pointcloud)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pcd_path = os.path.abspath(self.df.iloc[idx]['object_path'])\n",
        "        category = self.df.iloc[idx]['label']\n",
        "        pointcloud = self.__preproc__(pcd_path)\n",
        "        return {'pointcloud': pointcloud, \n",
        "                'category': self.classes[category]}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "opg-Z13sA3Zq"
      },
      "outputs": [],
      "source": [
        "# data_split = \"50-50\" \n",
        "train_df = pd.read_csv(\"./data/train_df.csv\")\n",
        "test_df = pd.read_csv(\"./data/test_df.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3g1m6I3SRu6",
        "outputId": "a4e3c43f-8352-4e83-f82c-0b1e2600dc2a"
      },
      "outputs": [],
      "source": [
        "# # Preprocessing step to change paths to Google Colab compatible paths\n",
        "\n",
        "# # Correcting paths to be Google Colab compatible\n",
        "# def preprocess_path(path, dataset_name):\n",
        "#   idx = path.rindex(dataset_name) \n",
        "#   replaced = path[idx:].replace(\"\\\\\", \"/\") \n",
        "#   return os.path.join(\"/content/\", replaced)\n",
        "\n",
        "\n",
        "# # Concatenate all dataset versions\n",
        "# train_df = pd.concat([pd.read_csv(f\"/content/{zip_name}/Var1/point_cloud_mesh/corrected_train_{data_split}_class_ration.csv\"),\n",
        "#                 pd.read_csv(f\"/content/{zip_name}/Var2/point_cloud_mesh/corrected_train_{data_split}_class_ration.csv\"),\n",
        "#                 pd.read_csv(f\"/content/{zip_name}/Var3/point_cloud_mesh/corrected_train_{data_split}_class_ration.csv\"),\n",
        "#                 pd.read_csv(f\"/content/{zip_name}/Var4/point_cloud_mesh/corrected_train_{data_split}_class_ration.csv\")])\n",
        "\n",
        "# test_df = pd.concat([pd.read_csv(f\"/content/{zip_name}/Var1/point_cloud_mesh/corrected_test_{data_split}_class_ration.csv\"),\n",
        "#                 pd.read_csv(f\"/content/{zip_name}/Var2/point_cloud_mesh/corrected_test_{data_split}_class_ration.csv\"),\n",
        "#                 pd.read_csv(f\"/content/{zip_name}/Var3/point_cloud_mesh/corrected_test_{data_split}_class_ration.csv\"),\n",
        "#                 pd.read_csv(f\"/content/{zip_name}/Var4/point_cloud_mesh/corrected_test_{data_split}_class_ration.csv\")])\n",
        "\n",
        "# train_df[\"object_pcd_path\"] = [preprocess_path(path, zip_name) for path in train_df[\"object_pcd_path\"].values]\n",
        "# train_df[\"object_path\"] = [preprocess_path(path, zip_name) for path in train_df[\"reference_path\"].values]\n",
        "# train_df[\"reference_path\"] = [preprocess_path(path, zip_name) for path in train_df[\"reference_path\"].values]\n",
        "\n",
        "# test_df[\"object_pcd_path\"] = [preprocess_path(path, zip_name) for path in test_df[\"object_pcd_path\"].values]\n",
        "# test_df[\"object_path\"] = [preprocess_path(path, zip_name) for path in test_df[\"object_path\"].values]\n",
        "# test_df[\"reference_path\"] = [preprocess_path(path, zip_name) for path in test_df[\"reference_path\"].values]\n",
        "\n",
        "# train_df.to_csv(\"/content/train_df.csv\", index=False)\n",
        "# test_df.to_csv(\"/content/test_df.csv\", index=False)\n",
        "\n",
        "\n",
        "# print(train_df.shape)\n",
        "# print(test_df.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tHoIJSxpiMnO"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2-NUNGPiNsF",
        "outputId": "616d7bf9-3446-47fd-ab44-79e65e66d5c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "sample_rate = 1024\n",
        "epochs = 10\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "pointnet = PointNet()\n",
        "pointnet.to(device)\n",
        "optimizer = torch.optim.Adam(pointnet.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "train_ds = PointCloudData(\"./data/train_df.csv\", sample_rate=sample_rate)\n",
        "train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "SZtp_WcsU55q"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, learning_rate, val_loader=None, epochs=5, save=True):\n",
        "      for epoch in range(epochs): \n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        batch_loss = []\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            pcd = data['pointcloud'].to(device).float()\n",
        "            labels = data['category'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs, m3x3, m64x64 = model(pcd.transpose(1,2))\n",
        "            loss = pointnetloss(outputs, labels, m3x3, m64x64, alpha=learning_rate)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:    # print every 10 mini-batches\n",
        "                    print('[Epoch: %d, Batch: %4d / %4d], loss: %.3f' %\n",
        "                        (epoch + 1, i + 1, len(train_loader), running_loss / 10))\n",
        "                    running_loss = 0.0\n",
        "\n",
        "        torch.save(model.state_dict(), f\"./model_weights/PointNet_\"+str(epoch)+\".pth\") # _90-10_sample_rate_{sample_rate}_batch_size_{batch_size}_lr_{learning_rate}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"['C:/Users/Sani/Desktop/WorkTest/mesh data/point_cloud_mesh\\\\\\\\anomaly\\\\\\\\anomaly_lattice_test4_v0.csv', 'C:/Users/Sani/Desktop/WorkTest/mesh data/point_cloud_mesh\\\\\\\\reference\\\\\\\\reference_lattice_test4.csv']\""
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df[\"object_path\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "72pNhijIVRKq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32\n"
          ]
        }
      ],
      "source": [
        "print(len(train_ds))\n",
        "train(pointnet, train_loader, learning_rate, save=True, epochs=epochs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zWoMMCnfVQUd"
      },
      "source": [
        "Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "cUEfhZ1QX5HZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "EPOCH: 0\n",
            "Batch [   1 /    1]\n",
            "[[0 4]\n",
            " [0 4]]\n",
            "****************************************\n",
            "cpu\n",
            "EPOCH: 1\n",
            "Batch [   1 /    1]\n",
            "[[0 4]\n",
            " [0 4]]\n",
            "****************************************\n",
            "cpu\n",
            "EPOCH: 2\n",
            "Batch [   1 /    1]\n",
            "[[0 4]\n",
            " [0 4]]\n",
            "****************************************\n",
            "cpu\n",
            "EPOCH: 3\n",
            "Batch [   1 /    1]\n",
            "[[4 0]\n",
            " [4 0]]\n",
            "****************************************\n",
            "cpu\n",
            "EPOCH: 4\n",
            "Batch [   1 /    1]\n",
            "[[4 0]\n",
            " [4 0]]\n",
            "****************************************\n",
            "cpu\n",
            "EPOCH: 5\n",
            "Batch [   1 /    1]\n",
            "[[4 0]\n",
            " [4 0]]\n",
            "****************************************\n",
            "cpu\n",
            "EPOCH: 6\n",
            "Batch [   1 /    1]\n",
            "[[4 0]\n",
            " [4 0]]\n",
            "****************************************\n",
            "cpu\n",
            "EPOCH: 7\n",
            "Batch [   1 /    1]\n",
            "[[4 0]\n",
            " [4 0]]\n",
            "****************************************\n",
            "cpu\n",
            "EPOCH: 8\n",
            "Batch [   1 /    1]\n",
            "[[4 0]\n",
            " [4 0]]\n",
            "****************************************\n",
            "cpu\n",
            "EPOCH: 9\n",
            "Batch [   1 /    1]\n",
            "[[4 0]\n",
            " [4 0]]\n",
            "****************************************\n"
          ]
        }
      ],
      "source": [
        "test_ds = PointCloudData(\"./data/test_df.csv\", valid=True)\n",
        "test_loader = DataLoader(dataset=test_ds, batch_size=32)\n",
        "for i in range(epochs):\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(device)\n",
        "  print(f\"EPOCH: {i}\")\n",
        "\n",
        "  pointnet = PointNet()\n",
        "  pointnet.to(device)\n",
        "  optimizer = torch.optim.Adam(pointnet.parameters(), lr=learning_rate)\n",
        "\n",
        "  pointnet.load_state_dict(torch.load(f\"./model_weights/PointNet_\"+str(i)+\".pth\"))\n",
        "  pointnet.eval();\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "  with torch.no_grad():\n",
        "      for i, data in enumerate(test_loader):\n",
        "          print('Batch [%4d / %4d]' % (i+1, len(test_loader)))\n",
        "                    \n",
        "          pcd = data['pointcloud'].to(device).float()\n",
        "          labels = data['category'].to(device)\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs, m3x3, m64x64 = pointnet(pcd.transpose(1,2))\n",
        "          _, preds = torch.max(outputs.data, 1)\n",
        "          all_preds += list(preds.cpu().numpy())\n",
        "          all_labels += list(labels.cpu().numpy())\n",
        "  cm = confusion_matrix(all_labels, all_preds);\n",
        "  print(cm)\n",
        "  print(\"*\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83ZjWa-Tz7nr",
        "outputId": "d17ea7e8-676f-49d7-ec7a-b27787f9c4cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "EPOCH: 0\n",
            "Batch [   1 /    1]\n",
            "[[0 4]\n",
            " [0 0]]\n",
            "cpu\n",
            "EPOCH: 1\n",
            "Batch [   1 /    1]\n",
            "[[0 4]\n",
            " [0 0]]\n",
            "cpu\n",
            "EPOCH: 2\n",
            "Batch [   1 /    1]\n",
            "[[0 4]\n",
            " [0 0]]\n",
            "cpu\n",
            "EPOCH: 3\n",
            "Batch [   1 /    1]\n",
            "[[4]]\n",
            "cpu\n",
            "EPOCH: 4\n",
            "Batch [   1 /    1]\n",
            "[[4]]\n",
            "cpu\n",
            "EPOCH: 5\n",
            "Batch [   1 /    1]\n",
            "[[4]]\n",
            "cpu\n",
            "EPOCH: 6\n",
            "Batch [   1 /    1]\n",
            "[[4]]\n",
            "cpu\n",
            "EPOCH: 7\n",
            "Batch [   1 /    1]\n",
            "[[4]]\n",
            "cpu\n",
            "EPOCH: 8\n",
            "Batch [   1 /    1]\n",
            "[[4]]\n",
            "cpu\n",
            "EPOCH: 9\n",
            "Batch [   1 /    1]\n",
            "[[4]]\n"
          ]
        }
      ],
      "source": [
        "# Loading 90/10 split created in random forest noteobok\n",
        "test_ds_90_10 = PointCloudData(\"./data/test_df_90_10.csv\", valid=True)\n",
        "test_loader_90_10 = DataLoader(dataset=test_ds_90_10, batch_size=batch_size)\n",
        "for i in range(epochs):\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(device)\n",
        "  print(f\"EPOCH: {i}\")\n",
        "\n",
        "  pointnet = PointNet()\n",
        "  pointnet.to(device)\n",
        "  optimizer = torch.optim.Adam(pointnet.parameters(), lr=learning_rate)\n",
        "\n",
        "  pointnet.load_state_dict(torch.load(f\"./model_weights/PointNet_\"+str(i)+\".pth\"))\n",
        "  pointnet.eval();\n",
        "  all_preds = []\n",
        "\n",
        "\n",
        "  all_preds_90_10 = []\n",
        "  all_labels_90_10 = []\n",
        "  with torch.no_grad():\n",
        "      for i, data in enumerate(test_loader_90_10):\n",
        "          print('Batch [%4d / %4d]' % (i+1, len(test_loader_90_10)))\n",
        "                    \n",
        "          pcd = data['pointcloud'].to(device).float()\n",
        "          labels = data['category'].to(device)\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs, m3x3, m64x64 = pointnet(pcd.transpose(1,2))\n",
        "          _, preds = torch.max(outputs.data, 1)\n",
        "          all_preds_90_10 += list(preds.cpu().numpy())\n",
        "          all_labels_90_10 += list(labels.cpu().numpy())\n",
        "\n",
        "  cm_90_10 = confusion_matrix(all_labels_90_10, all_preds_90_10);\n",
        "  print(cm_90_10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr0ERntMYp4o"
      },
      "source": [
        "# POINT NET + RandomForestSampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "1Lt_aIeaGkaI"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "sample_rate = 1024\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "1ssGyWq4gUp_"
      },
      "outputs": [],
      "source": [
        "def create_rf_point_samples(df, rf_point, sample_rate, name):\n",
        "  folder = \"./data/rf_point_samples_\"+name\n",
        "  os.makedirs(folder, exist_ok=True)\n",
        "  new_path_list = []\n",
        "  for idx, path in enumerate(df[\"reference_path\"]):\n",
        "    tmp_df = pd.read_csv(path)\n",
        "    # \n",
        "    preds = rf_point.predict_proba(tmp_df.loc[:, [\"x\", \"y\", \"z\", \"dist\"]].values)[:, 1]\n",
        "    sampled_inds = np.random.choice(range(tmp_df.shape[0]), size=sample_rate, p=softmax(preds), replace=False)\n",
        "    resampled_df = tmp_df.iloc[sampled_inds]\n",
        "    name = pathlib.Path(path).name\n",
        "    new_path = folder+\"/\"+ str(idx) + \".csv\"\n",
        "    resampled_df.loc[:, ['x', 'y', 'z']].to_csv(new_path, index=False)\n",
        "  \n",
        "    new_path_list.append(new_path)\n",
        "  df[\"sampled_pcds\"] = new_path_list\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "o0o_lQjX31tW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "\n",
        "# Load trained RF model for downsample point selection\n",
        "rf_point = load(\"./model_weights/rf_point.joblib\")\n",
        "\n",
        "RF_sampled_df = create_rf_point_samples(train_df, rf_point, sample_rate, \"train\")\n",
        "RF_sampled_df.to_csv(\"./data/RF_sampled_df.csv\", index=False)\n",
        "\n",
        "\n",
        "RF_sampled_df_test = create_rf_point_samples(test_df, rf_point, sample_rate, \"test\")\n",
        "RF_sampled_df_test.to_csv(\"./data/RF_sampled_df_test.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "2QW2YNH3axIo"
      },
      "outputs": [],
      "source": [
        "from scipy.special import softmax\n",
        "class PointCloudDataRFSampler(Dataset):\n",
        "    def __init__(self, dataframe_path, valid=False, sample_rate=1024, transform=default_transforms()):\n",
        "        self.df = pd.read_csv(dataframe_path)\n",
        "        self.classes = {\"anomaly\": 1, \"normal\": 0}\n",
        "        self.sample_rate=sample_rate\n",
        "        self.transforms=transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __preproc__(self, file, reference_path):\n",
        "        # Loads point cloud\n",
        "        resampled = pd.read_csv(file).values\n",
        "        # Apply transforms (only normalize but additional can be added)\n",
        "        if self.transforms:\n",
        "            pointcloud = self.transforms(resampled)\n",
        "        return torch.from_numpy(pointcloud)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pcd_path = self.df.iloc[idx]['sampled_pcds']\n",
        "        ref_path = self.df.iloc[idx]['reference_path']\n",
        "        category = self.df.iloc[idx]['label']\n",
        "        pointcloud = self.__preproc__(pcd_path, ref_path)\n",
        "        return {'pointcloud': pointcloud, \n",
        "                'category': self.classes[category]}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOdMzsc_M2pp",
        "outputId": "60dc326e-f576-47c2-83e4-c9853e4c6c33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "pointnet = PointNet()\n",
        "pointnet.to(device)\n",
        "optimizer = torch.optim.Adam(pointnet.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_dLcKrRzLpj"
      },
      "outputs": [],
      "source": [
        "# train_sample_df = pd.read_csv(\"./data/RF_sampled_df.csv\")\n",
        "# train_sample_df[\"sampled_pcds\"] = [\"/content\"+name for name in train_sample_df[\"sampled_pcds\"]]\n",
        "# train_sample_df.to_csv(\"./data/RF_sampled_df.csv\",index=False)\n",
        "# test_sample_df = pd.read_csv(\"RF_sampled_df_test.csv\")\n",
        "# test_sample_df[\"sampled_pcds\"] = [\"/content\"+name for name in test_sample_df[\"sampled_pcds\"]]\n",
        "# test_sample_df.to_csv(\"RF_sampled_df_test.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "sZDrAKNoM3Ru"
      },
      "outputs": [],
      "source": [
        "train_point_ds = PointCloudDataRFSampler(\"./data/RF_sampled_df.csv\", sample_rate=sample_rate)\n",
        "train_loader = DataLoader(train_point_ds, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "rZ5E08guM3b-"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, learning_rate, val_loader=None, epochs=5, save=True):\n",
        "      for epoch in range(epochs): \n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            pcd = data['pointcloud'].to(device).float()\n",
        "            labels = data['category'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs, m3x3, m64x64 = model(pcd.transpose(1,2))\n",
        "            loss = pointnetloss(outputs, labels, m3x3, m64x64, alpha=learning_rate)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:    # print every 10 mini-batches\n",
        "                    print('[Epoch: %d, Batch: %4d / %4d], loss: %.3f' %\n",
        "                        (epoch + 1, i + 1, len(train_loader), running_loss / 10))\n",
        "                    running_loss = 0.0\n",
        "        torch.save(model.state_dict(), f\"./model_weights/PointNet_RF_samples_\"+str(epoch)+\".pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMTD7wqEM3fu",
        "outputId": "745f4acb-92dc-4674-e7bd-3340ddf0450e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32\n"
          ]
        }
      ],
      "source": [
        "print(len(train_point_ds))\n",
        "train(pointnet, train_loader, learning_rate, save=True, epochs=epochs)\n",
        "# !cp \"/content/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_0.pth\" \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)\"\n",
        "# !cp \"/content/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_1.pth\" \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)\"\n",
        "# !cp \"/content/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_2.pth\" \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)\"\n",
        "# !cp \"/content/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_3.pth\" \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)\"\n",
        "# !cp \"/content/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_4.pth\" \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)\"\n",
        "# !cp \"/content/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_5.pth\" \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)\"\n",
        "# !cp \"/content/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_6.pth\" \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)\"\n",
        "# !cp \"/content/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_7.pth\" \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)\"\n",
        "# !cp \"/content/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_8.pth\" \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)\"\n",
        "# !cp \"/content/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_9.pth\" \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "p3G55ILAsyyz"
      },
      "source": [
        "### EVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaBlYB5bs2VF"
      },
      "outputs": [],
      "source": [
        "# !cp \"drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)/RF Samples/RF_sampled_df_test.csv\" \"/content/\"\n",
        "# !cp \"drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)/RF Samples/rf_point_samples_test.zip\" \"/content/\"\n",
        "\n",
        "# !cp \"drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)/RF Samples/rf_point_samples_train.zip\" \"/content/\"\n",
        "# !cp \"drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)/RF Samples/RF_sampled_df.csv\" \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Pbjoh3EgwhEn"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "sample_rate = 1024\n",
        "epochs = 10\n",
        "\n",
        "test_point_ds = PointCloudDataRFSampler(\"./data/RF_sampled_df_test.csv\", sample_rate=sample_rate)\n",
        "test_loader = DataLoader(test_point_ds, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjgITMmP8S4E"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !cp \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_0.pth\" \"/content/\"\n",
        "# !cp \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_1.pth\" \"/content/\"\n",
        "# !cp \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_2.pth\" \"/content/\"\n",
        "# !cp \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_3.pth\" \"/content/\"\n",
        "# !cp \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_4.pth\" \"/content/\"\n",
        "# !cp \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_5.pth\" \"/content/\"\n",
        "# !cp \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_6.pth\" \"/content/\"\n",
        "# !cp \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_7.pth\" \"/content/\"\n",
        "# !cp \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_8.pth\" \"/content/\"\n",
        "# !cp \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (RF)/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_9.pth\" \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz-I6cosw4E9",
        "outputId": "6773863c-b30b-4b27-fcd3-c325d4601118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "EPOCH: 0\n",
            "Batch [   1 /    1]\n",
            "[[0 4]\n",
            " [0 4]]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 1\n",
            "Batch [   1 /    1]\n",
            "[[0 4]\n",
            " [0 4]]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 2\n",
            "Batch [   1 /    1]\n",
            "[[0 4]\n",
            " [0 4]]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 3\n",
            "Batch [   1 /    1]\n",
            "[[4 0]\n",
            " [4 0]]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 4\n",
            "Batch [   1 /    1]\n",
            "[[4 0]\n",
            " [4 0]]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 5\n",
            "Batch [   1 /    1]\n",
            "[[4 0]\n",
            " [4 0]]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 6\n",
            "Batch [   1 /    1]\n",
            "[[4 0]\n",
            " [4 0]]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 7\n",
            "Batch [   1 /    1]\n",
            "[[4 0]\n",
            " [4 0]]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 8\n",
            "Batch [   1 /    1]\n",
            "[[4 0]\n",
            " [4 0]]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 9\n",
            "Batch [   1 /    1]\n",
            "[[4 0]\n",
            " [4 0]]\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for i in range(0,10):\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(device)\n",
        "\n",
        "  pointnet = PointNet()\n",
        "  pointnet.to(device)\n",
        "  optimizer = torch.optim.Adam(pointnet.parameters(), lr=learning_rate)\n",
        "\n",
        "  pointnet.load_state_dict(torch.load(f\"./model_weights//PointNet_{i}.pth\"))\n",
        "\n",
        "  pointnet.eval();\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "  print(f\"EPOCH: {i}\")\n",
        "  with torch.no_grad():\n",
        "      for i, data in enumerate(test_loader):\n",
        "          print('Batch [%4d / %4d]' % (i+1, len(test_loader)))\n",
        "                    \n",
        "          pcd = data['pointcloud'].to(device).float()\n",
        "          labels = data['category'].to(device)\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs, m3x3, m64x64 = pointnet(pcd.transpose(1,2))\n",
        "          _, preds = torch.max(outputs.data, 1)\n",
        "          all_preds += list(preds.cpu().numpy())\n",
        "          all_labels += list(labels.cpu().numpy())\n",
        "  cm = confusion_matrix(all_labels, all_preds);\n",
        "  print(cm)\n",
        "  print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7yUQssB0Uif",
        "outputId": "704cf71f-607d-444e-fc58-6a8d6e3c3dc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "EPOCH: 0\n",
            "[]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 1\n",
            "[]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 2\n",
            "[]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 3\n",
            "[]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 4\n",
            "[]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 5\n",
            "[]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 6\n",
            "[]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 7\n",
            "[]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 8\n",
            "[]\n",
            "========================================\n",
            "cpu\n",
            "EPOCH: 9\n",
            "[]\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "# !cp \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/RandomForestData/test_df_90_10.csv\" \"/content/\"\n",
        "\n",
        "test_90_10 = pd.read_csv(\"./data/test_df_90_10.csv\")\n",
        "test_df = pd.read_csv(\"./data/RF_sampled_df_test.csv\")\n",
        "inds = []\n",
        "for row in test_df.iterrows():\n",
        "  if row[1][\"object_path\"] in test_90_10[\"object_path\"].tolist():\n",
        "    inds.append(row[0])\n",
        "\n",
        "len(inds)\n",
        "# Match indices of 90/10 dataset with 50/50 full dataset of points\n",
        "# Save train dataset\n",
        "test_df.iloc[inds].to_csv(\"test_90_10.csv\", index=False)\n",
        "\n",
        "test_point_ds_90_10 = PointCloudDataRFSampler(\"test_90_10.csv\", sample_rate=sample_rate)\n",
        "test_loader_90_10 = DataLoader(test_point_ds_90_10, batch_size=batch_size)\n",
        "\n",
        "for i in range(0,10):\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(device)\n",
        "\n",
        "  pointnet = PointNet()\n",
        "  pointnet.to(device)\n",
        "  optimizer = torch.optim.Adam(pointnet.parameters(), lr=learning_rate)\n",
        "\n",
        "  pointnet.load_state_dict(torch.load(f\"/content/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_{i}.pth\"))\n",
        "\n",
        "  pointnet.eval();\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "  print(f\"EPOCH: {i}\")\n",
        "  with torch.no_grad():\n",
        "      for i, data in enumerate(test_loader_90_10):\n",
        "          print('Batch [%4d / %4d]' % (i+1, len(test_loader_90_10)))\n",
        "                    \n",
        "          pcd = data['pointcloud'].to(device).float()\n",
        "          labels = data['category'].to(device)\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          outputs, m3x3, m64x64 = pointnet(pcd.transpose(1,2))\n",
        "          _, preds = torch.max(outputs.data, 1)\n",
        "          all_preds += list(preds.cpu().numpy())\n",
        "          all_labels += list(labels.cpu().numpy())\n",
        "  cm = confusion_matrix(all_labels, all_preds);\n",
        "  print(cm)\n",
        "  print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
