{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TgXocVJxA5Wf"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tQ-IhssY2OLT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "from joblib import dump\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "from notebook_utils.propensity_utils import get_reference_pairs, generate_distance_csvs\n",
        "\n",
        "\n",
        "random.seed = 42"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yK29Octx2e64"
      },
      "source": [
        "## Point model data preparation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate 2 datasets using script.\n",
        "Make sure the datasets have different paths when generating and that the parameters for anomalies are the same\n",
        "\n",
        "One dataset is for training the Random Forest propensity model\n",
        "The second one used for per mesh classification"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function is used to generate a dataframe used for training per propensity model.\n",
        "<ul>\n",
        "<li><b>id</b> - unique mesh identifier</li>\n",
        "<li><b>dist</b> - distance value of item point to relevant reference mesh point</li>\n",
        "<li><b>label</b> - per point label for whether the point is on an anomaly</li>\n",
        "<li><b>label_obj</b> - whether the mesh contains an anomaly</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load csv files with per point values of ['x', 'y', 'z', 'dist', 'label']\n",
        "def get_samples(paths, labels):\n",
        "  # Create point dataframe\n",
        "  columns_list = ['id', 'x', 'y', 'z', 'dist', 'label', 'label_obj']\n",
        "  point_df = pd.DataFrame(columns=columns_list)\n",
        "  for idx, (name, label) in enumerate(zip(paths, labels)):\n",
        "      if name.endswith(\".csv\"):\n",
        "\n",
        "        # Loading point dataframe of single point cloud \n",
        "        df_tmp = pd.read_csv(name)\n",
        "        df_tmp['id'] = idx\n",
        "        df_tmp['label_obj'] = 0 if label==\"normal\" else 1\n",
        "        \n",
        "        # Concatenate to single dataframe\n",
        "        point_df = pd.concat([point_df, df_tmp], ignore_index=True)                \n",
        "  return point_df\n",
        "\n",
        "def data_split(paths_anom, paths_norm, test_split, point=True):\n",
        "  '''\n",
        "  Function used for splitting point dataset\n",
        "  '''\n",
        "  # Calculate data split\n",
        "  split_ids = int(len(paths_anom) * test_split)\n",
        "  print(f\"Anoms: {len(paths_anom)}\")\n",
        "  print(f\"Norms: {len(paths_norm)}\")\n",
        "\n",
        "  # Split train and test paths per class\n",
        "  train_paths_anom = paths_anom[split_ids:]\n",
        "  train_paths_norm = paths_norm[split_ids:]\n",
        "  test_paths_anom = paths_anom[:split_ids]\n",
        "  test_paths_norm = paths_norm[:split_ids]\n",
        "\n",
        "  train_labels = [1 for _ in range(len(train_paths_anom))] + [0 for _ in range(len(train_paths_norm))]\n",
        "  test_labels = [1 for _ in range(len(test_paths_anom))] + [0 for _ in range(len(test_paths_norm))]\n",
        "  train_point_df = get_samples(train_paths_anom + train_paths_norm, train_labels)\n",
        "  train_point_df.to_csv(\"./data/train_point_df.csv\", index=False)\n",
        "\n",
        "  # Test dataset\n",
        "  test_point_df = get_samples(test_paths_anom + test_paths_norm, test_labels)\n",
        "  test_point_df.to_csv(\"./data/test_point_df.csv\", index=False)\n",
        "\n",
        "  # 90/10 class split\n",
        "  anom_split_90_10 = int(len(test_paths_anom)/9)\n",
        "  test_paths_anom_90_10 = test_paths_anom[:anom_split_90_10]\n",
        "  test_labels_90_10 = [1 for _ in range(len(test_paths_anom_90_10))] + [0 for _ in range(len(test_paths_norm))]\n",
        "  # 90-10 split dataset\n",
        "  test_point_df_90_10 = get_samples(test_paths_anom_90_10 + test_paths_norm, test_labels_90_10)\n",
        "  test_point_df_90_10.to_csv(\"./data/test_point_df_90_10.csv\", index=False)\n",
        "\n",
        "  return train_point_df, test_point_df, test_point_df_90_10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "paths_of_point_clouds = \"/point_cloud_mesh/\" # Insert path to point cloud folder of propensity dataset\n",
        "test_split = 0.2 # test split for propensity data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Get matching reference to anomaly/normal mesh\n",
        "pairs_of_paths = get_reference_pairs(\n",
        "    {\"anomaly\": [], 'normal': []},\n",
        "    paths_of_point_clouds\n",
        ")\n",
        "# Generate distance csvs to be used for propensity model training\n",
        "df_point = generate_distance_csvs(\n",
        "    pairs_of_paths,\n",
        "    os.path.join(paths_of_point_clouds, \"generate_distance_csvs\")\n",
        ")\n",
        "train_point_df, test_point_df, test_point_df_90_10 = data_split(df_point[\"anomaly\"], df_point[\"normal\"], test_split)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pjr0uEv4Qx6a"
      },
      "source": [
        "### Point anomaly dataset\n",
        "\n",
        "Point anomaly dataset contains csv files with columns [\"x\", \"y\", \"z\", \"dist\"]\n",
        "where dist is the per point distance to the relevant mesh point\n",
        "\n",
        "This dataset is used for training propensity random forest model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y8KMHcj13nxR"
      },
      "source": [
        "## Point model training\n",
        "\n",
        "To train propensity random forest model should be loaded in the following cell.\n",
        "Set contains [\"x\", \"y\", \"z\", \"dist\", \"id\", \"label_obj\", \"label\"] values per point for all meshes.\n",
        "\n",
        "**id** - unique mesh id\n",
        "\n",
        "**label** - 1 if point is on anomaly else 0\n",
        "\n",
        "**label_obj** - 1 if mesh has anomaly else 0 for all points in mesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5BNVDRvmpP8",
        "outputId": "b35bde49-ef5b-4f6f-83ee-ccd313301cc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1287744, 7)\n",
            "(321936, 7)\n",
            "(160968, 7)\n"
          ]
        }
      ],
      "source": [
        "# Load dataframes for point net training\n",
        "train_point_df = pd.read_csv(\"./data/train_point_df.csv\")\n",
        "test_point_df = pd.read_csv(\"./data/test_point_df.csv\")\n",
        "test_point_df_90_10 = pd.read_csv(\"./data/test_point_df_90_10.csv\")\n",
        "print(train_point_df.shape)\n",
        "print(test_point_df.shape)\n",
        "print(test_point_df_90_10.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QcHsg-4AWnUU"
      },
      "outputs": [],
      "source": [
        "# Fraction of ids to be used for training\n",
        "frac_of_ids = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "_-Oiio7xUk1A",
        "outputId": "7c83668f-d803-4d21-c2b0-53dd0940a498"
      },
      "outputs": [],
      "source": [
        "selected_ids = np.random.choice(\n",
        "    train_point_df['id'].unique(), \n",
        "    int(frac_of_ids*len(train_point_df['id'].unique())), \n",
        "    replace=False\n",
        ")\n",
        "# Select subsample for Random Forest point training\n",
        "df_anomaly_point = train_point_df.loc[train_point_df.id.isin(selected_ids)]\n",
        "#  Save dataframe for reproducibility\n",
        "df_anomaly_point.to_csv(\"./data/selected_train_points.csv\")\n",
        "\n",
        "# Train Random Forest point model\n",
        "features = df_anomaly_point.loc[:, ['x','y','z','dist']].values \n",
        "labels =  df_anomaly_point.loc[:, 'label'].values \n",
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
        "rf_point = RandomForestClassifier(verbose=1, n_jobs=2) \n",
        "rf_point.fit(features, labels)\n",
        "\n",
        "# Plot results of model on test data sample\n",
        "test_features = test_point_df.loc[:, ['x','y','z','dist']].values \n",
        "test_labels =  test_point_df.loc[:, 'label'].values \n",
        "test_pred = rf_point.predict(test_features)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "cm = confusion_matrix(test_labels, test_pred)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm).plot()\n",
        "print(cm)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vyP6rru7Wqnc"
      },
      "source": [
        "### Model weights transfered to drive \n",
        "Transfer to own drive for it to be used for PointNet if RF point sampling is to be used for predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "V7yqn6YO3Qhu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['./model_weights/rf_point.joblib']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save point model\n",
        "model_save_path = \"./model_weights\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "dump(rf_point, f'{model_save_path}/rf_point.joblib') "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6bFXhfr0R29x"
      },
      "source": [
        "Test 90-10 evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "v_Ahkof6R16j",
        "outputId": "cd9d1891-c644-48bb-cf69-bc20145d48a0"
      },
      "outputs": [],
      "source": [
        "# Plot results of model on test data sample\n",
        "test_features_90_10 = test_point_df_90_10.loc[:, ['x','y','z','dist']].values \n",
        "test_labels_90_10 =  test_point_df_90_10.loc[:, 'label'].values \n",
        "test_pred_90_10 = rf_point.predict(test_features_90_10)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "cm_90_10 = confusion_matrix(test_labels_90_10, test_pred_90_10)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_90_10).plot()\n",
        "print(cm_90_10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AZBGVacb3zMF"
      },
      "source": [
        "# Point cloud model dataset preparation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QTb5zWSJy9A1"
      },
      "source": [
        "Once point model is trained we train mesh classification model on the mesh dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5bGN-hlAzPkg"
      },
      "source": [
        "### Point aggregation\n",
        "\n",
        "For the mesh we generate a separate dataset where we use the trained rf_point model to predict the porability of anomalies per point.\n",
        "\n",
        "We then use those predictions and the distance metrics to get percentiles for each mesh to get input features for our random forest classifier\n",
        "Each row represents a mesh with label_obj_is_anom as label for the mesh and the different percentiles we calculate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here we create dataframe from second dataset\n",
        "path_mesh_point_cloud = \"C:/Users/Sani/Desktop/WorkTest/mesh data/point_cloud_mesh\"\n",
        "\n",
        "# Get matching reference to anomaly/normal mesh\n",
        "pairs_of_paths_mesh = get_reference_pairs(\n",
        "    {\"anomaly\": [], 'normal': []},\n",
        "    path_mesh_point_cloud\n",
        ")\n",
        "# Generate distance csvs to be used for propensity model training\n",
        "df = generate_distance_csvs(\n",
        "    pairs_of_paths_mesh,\n",
        "    os.path.join(path_mesh_point_cloud, \"generate_distance_csvs\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_mesh_dataframe(pairs_of_paths_mesh, df):\n",
        "    '''\n",
        "    Function used for creating dataframe for easier data\n",
        "    manipulation with dataset\n",
        "    '''\n",
        "    data_dict = {\"object_path\": [], \"reference_path\": [], \"label\": []}\n",
        "    for key in [\"anomaly\", \"normal\"]:\n",
        "        for i in range(len(df[key])):\n",
        "            data_dict[\"object_path\"].append(pairs_of_paths_mesh[key][i][0].replace(\"csv\", \"pcd\"))\n",
        "            data_dict[\"reference_path\"].append(df[key][i])\n",
        "            data_dict[\"label\"].append(key)\n",
        "    return pd.DataFrame(data_dict)\n",
        "\n",
        "def create_mesh_split(pairs_of_paths_mesh, df, test_split):\n",
        "    '''Function used for creating full mesh dataset\n",
        "    data splits including train, test and 90/10 class \n",
        "    ratio'''\n",
        "    df = create_mesh_dataframe(pairs_of_paths_mesh, df)\n",
        "\n",
        "    df_anom = df[df[\"label\"] == \"anomaly\"]\n",
        "    df_norm = df[df[\"label\"] == \"normal\"]\n",
        "\n",
        "    split_idx = int(df_anom.shape[0] * test_split)\n",
        "\n",
        "    train_df = pd.concat([\n",
        "        df_anom.iloc[split_idx:],\n",
        "        df_norm.iloc[split_idx:]\n",
        "\n",
        "    ], ignore_index=False)\n",
        "\n",
        "    test_df = pd.concat([\n",
        "        df_anom.iloc[:split_idx],\n",
        "        df_norm.iloc[:split_idx]\n",
        "    ], ignore_index=False)\n",
        "\n",
        "    test_df_90_10 = pd.concat([\n",
        "        df_anom.iloc[:int(split_idx/9)],\n",
        "        df_norm.iloc[:split_idx]\n",
        "    ], ignore_index=False)\n",
        "    train_df.to_csv(\"./data/train_df.csv\", index=False)\n",
        "    test_df.to_csv(\"./data/test_df.csv\", index=False)\n",
        "    test_df_90_10.to_csv(\"./data/test_df_90_10.csv\", index=False)\n",
        "\n",
        "    return train_df, test_df, test_df_90_10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train test split per mesh ratio\n",
        "\n",
        "mesh_test_split = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df, test_df, test_point_df_90_10 = create_mesh_split(pairs_of_paths_mesh, df, mesh_test_split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "kqs1bpEo3_Vz"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Aggregate data for training the model to detect anomaly mesh\n",
        "# This cell needs to be run to create aggregated dataset for use\n",
        "# by RandomForest+PointNet model\n",
        "\n",
        "# Calculating percentile of items per dataset\n",
        "def percentile(n):\n",
        "  \"\"\"\n",
        "  Calculates percentile values per mesh in dataframe\n",
        "  \"\"\"\n",
        "  def percentile_(x):\n",
        "      return np.percentile(x, n)\n",
        "  percentile_.__name__ = 'percentile_%s' % n\n",
        "  return percentile_\n",
        "\n",
        "\n",
        "def is_anom(series):\n",
        "  \"\"\"\n",
        "  Function used for labelling entire dataset maximum value\n",
        "  of label_obj column\n",
        "  \"\"\"\n",
        "  # If maximum value is set to anomaly\n",
        "  return series.max()\n",
        "\n",
        "def aggregate_data(df):\n",
        "  \"\"\"\n",
        "  Function used to aggregate points per mesh into\n",
        "  1 row with percentile information for dist and prob_anom\n",
        "  columns columns and preserves label_obj as label for classification\n",
        "  \"\"\"\n",
        "  aggr_df = df.groupby(['id']).agg(\n",
        "      {'label_obj': is_anom,\n",
        "      'dist':[\n",
        "          'mean',\n",
        "          'max',\n",
        "          percentile(60),\n",
        "          percentile(70),\n",
        "          percentile(80),\n",
        "          percentile(90),\n",
        "          percentile(95),\n",
        "          percentile(96),\n",
        "          percentile(97),\n",
        "          percentile(98),\n",
        "          percentile(99),\n",
        "      ],\n",
        "      'prob_anom':[\n",
        "          'mean',  \n",
        "          'max', \n",
        "          percentile(60),\n",
        "          percentile(70),\n",
        "          percentile(80),\n",
        "          percentile(90),\n",
        "          percentile(95),\n",
        "          percentile(96),\n",
        "          percentile(97),\n",
        "          percentile(98),\n",
        "          percentile(99),\n",
        "      ],\n",
        "      }\n",
        "  )\n",
        "  aggr_df.columns = ['_'.join(col) for col in aggr_df.columns.values]\n",
        "  return aggr_df\n",
        "\n",
        "def create_aggregated_dataset(df, rf_point):\n",
        "  '''\n",
        "  Function takes dataframe and trained random forest model\n",
        "  to create aggregate dataframe based on all points in a mesh\n",
        "  '''\n",
        "  # Load test data\n",
        "  dataset = None\n",
        "  for idx, (name, label) in enumerate(zip(df[\"reference_path\"], df[\"label\"])):\n",
        "      if name.endswith(\".csv\"):\n",
        "        # Loading dataframe of file\n",
        "          df_tmp = pd.read_csv(name)\n",
        "          df_tmp['id'] = idx\n",
        "          df_tmp['label_obj'] = 1 if label == \"anomaly\" else 0\n",
        "\n",
        "          df_tmp[\"prob_anom\"] = rf_point.predict_proba(df_tmp.loc[:, ['x', 'y', 'z', 'dist']].values)[:,1]\n",
        "          # Selecting uniformly distributed indices\n",
        "          if isinstance(dataset, type(None)):\n",
        "              dataset = aggregate_data(df_tmp)\n",
        "          else:\n",
        "              dataset = pd.concat([dataset,\n",
        "                                aggregate_data(df_tmp)],\n",
        "                                ignore_index=True)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "train_anomaly_mesh = create_aggregated_dataset(train_df, rf_point)\n",
        "print(\"Train done\")\n",
        "test_anomaly_mesh = create_aggregated_dataset(test_df, rf_point)\n",
        "print(\"Test done\")\n",
        "print(train_anomaly_mesh[\"label_obj_is_anom\"].value_counts())\n",
        "print(test_anomaly_mesh[\"label_obj_is_anom\"].value_counts())\n",
        "\n",
        "# Save aggregated dataset for further use of RandomForest+PointNet\n",
        "\n",
        "train_anomaly_mesh.to_csv(\"./data/aggregated_train.csv\", index=False) \n",
        "test_anomaly_mesh.to_csv(\"./data/aggregated_test.csv\", index=False) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R5wprB8wKvRw"
      },
      "source": [
        "Random Forest training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "eYzVsbdm4VTf",
        "outputId": "fb38d754-1af1-48b3-e8f1-ddd36da3dab2"
      },
      "outputs": [],
      "source": [
        "# Train model for random forest classifier for detecting anomalous meshes\n",
        "\n",
        "train_features_mesh = train_anomaly_mesh.loc[:, train_anomaly_mesh.columns[1:]].values\n",
        "train_labels_mesh = train_anomaly_mesh.loc[:, 'label_obj_is_anom'].values.astype(int)\n",
        "\n",
        "test_features_mesh = test_anomaly_mesh.loc[:, train_anomaly_mesh.columns[1:]].values\n",
        "test_labels_mesh = test_anomaly_mesh.loc[:, 'label_obj_is_anom'].values.astype(int)\n",
        "\n",
        "# Fit model\n",
        "rf_mesh = RandomForestClassifier()\n",
        "rf_mesh.fit(train_features_mesh, train_labels_mesh)\n",
        "\n",
        "test_pred_mesh = rf_mesh.predict(test_features_mesh)\n",
        "# Create the confusion matrix to plot results\n",
        "cm = confusion_matrix(test_labels_mesh, test_pred_mesh)\n",
        "\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm).plot()\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "l4CDlEec6J6A"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['./model_weights/rf_mesh.joblib']"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save model for use in PointNet with RF downsampling selection\n",
        "dump(rf_mesh, f'{model_save_path}/rf_mesh.joblib') "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KfH8Y1pILjUM"
      },
      "source": [
        "90-10 anomaly split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "Ix5ufT9THVce",
        "outputId": "d20d4ec3-2753-44bc-ad9a-309d7876476e"
      },
      "outputs": [],
      "source": [
        "# Results for 90/10 split of dataset\n",
        "\n",
        "# Create the confusion matrix to plot results\n",
        "test_point_df_90_10 = pd.read_csv(\"./data/test_df_90_10.csv\")\n",
        "\n",
        "# Match indices of selected 90/10 dataframe\n",
        "inds = [idx for idx, name in enumerate(test_df[\"reference_path\"]) if name in test_point_df_90_10[\"reference_path\"].tolist()]\n",
        "test_anomaly_mesh_90_10 = test_anomaly_mesh.iloc[inds]\n",
        "\n",
        "\n",
        "test_features_mesh_90_10 = test_anomaly_mesh_90_10.loc[:, test_anomaly_mesh_90_10.columns[1:]].values\n",
        "test_labels_mesh_90_10 = test_anomaly_mesh_90_10.loc[:, 'label_obj_is_anom'].values.astype(int)\n",
        "test_pred_mesh_90_10 = rf_mesh.predict(test_features_mesh_90_10)\n",
        "\n",
        "cm = confusion_matrix(test_labels_mesh_90_10, test_pred_mesh_90_10)\n",
        "\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm).plot()\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
