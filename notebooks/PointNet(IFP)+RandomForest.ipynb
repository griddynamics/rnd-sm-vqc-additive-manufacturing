{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--NMQPXB-x69"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import open3d as o3d\n",
        "import zipfile\n",
        "import glob\n",
        "from joblib import load, dump\n",
        "import pathlib\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import scipy.spatial.distance\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torch import nn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "random.seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jFLocDPQj1P"
      },
      "outputs": [],
      "source": [
        "# Load point model\n",
        "rf_point = load(\"./model_weights/rf_point.joblib\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ieBe7Nrq-6lc"
      },
      "source": [
        "## PointNet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izCIumko-51a"
      },
      "outputs": [],
      "source": [
        "# Used for extracting feature rich vector giving a 1 dimensional vector for point net\n",
        "\n",
        "class Tnet(nn.Module):\n",
        "   def __init__(self, k=3):\n",
        "      super().__init__()\n",
        "      self.k=k\n",
        "      self.conv1 = nn.Conv1d(k,64,1)\n",
        "      self.conv2 = nn.Conv1d(64,128,1)\n",
        "      self.conv3 = nn.Conv1d(128,1024,1)\n",
        "      self.fc1 = nn.Linear(1024,512)\n",
        "      self.fc2 = nn.Linear(512,256)\n",
        "      self.fc3 = nn.Linear(256,k*k)\n",
        "\n",
        "      self.bn1 = nn.BatchNorm1d(64)\n",
        "      self.bn2 = nn.BatchNorm1d(128)\n",
        "      self.bn3 = nn.BatchNorm1d(1024)\n",
        "      self.bn4 = nn.BatchNorm1d(512)\n",
        "      self.bn5 = nn.BatchNorm1d(256)\n",
        "       \n",
        "\n",
        "   def forward(self, input):\n",
        "      # input.shape == (bs,n,3)\n",
        "      bs = input.size(0)\n",
        "      xb = F.relu(self.bn1(self.conv1(input)))\n",
        "      xb = F.relu(self.bn2(self.conv2(xb)))\n",
        "      xb = F.relu(self.bn3(self.conv3(xb)))\n",
        "      pool = nn.MaxPool1d(xb.size(-1))(xb)\n",
        "      flat = nn.Flatten(1)(pool)\n",
        "      xb = F.relu(self.bn4(self.fc1(flat)))\n",
        "      xb = F.relu(self.bn5(self.fc2(xb)))\n",
        "      \n",
        "      #initialize as identity\n",
        "      init = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)\n",
        "      if xb.is_cuda:\n",
        "        init=init.cuda()\n",
        "      matrix = self.fc3(xb).view(-1,self.k,self.k) + init\n",
        "      return matrix\n",
        "\n",
        "# Used for position estimation and point estimation using global and\n",
        "# local coordinates\n",
        "class Transform(nn.Module):\n",
        "   def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input_transform = Tnet(k=3)\n",
        "        self.feature_transform = Tnet(k=64)\n",
        "        self.conv1 = nn.Conv1d(3,64,1)#(3,64,1)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(64,128,1)\n",
        "        self.conv3 = nn.Conv1d(128,1024,1)\n",
        "       \n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "       \n",
        "   def forward(self, input):\n",
        "        matrix3x3 = self.input_transform(input)\n",
        "        # batch matrix multiplication\n",
        "        xb = torch.bmm(torch.transpose(input,1,2), matrix3x3).transpose(1,2)\n",
        "\n",
        "        xb = F.relu(self.bn1(self.conv1(xb)))\n",
        "\n",
        "        matrix64x64 = self.feature_transform(xb)\n",
        "        xb = torch.bmm(torch.transpose(xb,1,2), matrix64x64).transpose(1,2)\n",
        "\n",
        "        xb = F.relu(self.bn2(self.conv2(xb)))\n",
        "        xb = self.bn3(self.conv3(xb))\n",
        "        xb = nn.MaxPool1d(xb.size(-1))(xb)\n",
        "        output = nn.Flatten(1)(xb)\n",
        "        return output, matrix3x3, matrix64x64\n",
        "\n",
        "class PointNet(nn.Module):\n",
        "    def __init__(self, classes = 2):\n",
        "        super().__init__()\n",
        "        self.transform = Transform()\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, classes)\n",
        "        \n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        xb, matrix3x3, matrix64x64 = self.transform(input)\n",
        "        xb1 = F.relu(self.bn1(self.fc1(xb)))\n",
        "        xb2 = F.relu(self.bn2(self.dropout(self.fc2(xb1))))\n",
        "        output = self.fc3(xb2)\n",
        "        return self.logsoftmax(output), matrix3x3, matrix64x64, xb1, xb2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4Euhnug_BYg"
      },
      "outputs": [],
      "source": [
        "# PointNet custom loss calculating with all 3 model outputs\n",
        "\n",
        "# LOSS\n",
        "def pointnetloss(outputs, labels, m3x3, m64x64, alpha = 0.0001):\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "    bs = outputs.size(0)\n",
        "    id3x3 = torch.eye(3, requires_grad=True).repeat(bs, 1, 1)\n",
        "    id64x64 = torch.eye(64, requires_grad=True).repeat(bs, 1, 1)\n",
        "    if outputs.is_cuda:\n",
        "        id3x3 = id3x3.cuda()\n",
        "        id64x64 = id64x64.cuda()\n",
        "    diff3x3 = id3x3 - torch.bmm(m3x3, m3x3.transpose(1, 2))\n",
        "    diff64x64 = id64x64 - torch.bmm(m64x64, m64x64.transpose(1, 2))\n",
        "    return criterion(outputs, labels) + alpha * (torch.norm(diff3x3) + torch.norm(diff64x64)) / float(bs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xK0YvUKA_fOZ"
      },
      "source": [
        "### Data loader for PointNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SmotiDL76MK"
      },
      "outputs": [],
      "source": [
        "# Normalization used as a preprocessing step\n",
        "class Normalize(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        assert len(pointcloud.shape)==2\n",
        "        \n",
        "        norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0) \n",
        "        norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))\n",
        "\n",
        "        return  norm_pointcloud\n",
        "\n",
        "def default_transforms():\n",
        "    return transforms.Compose([\n",
        "                                Normalize(),\n",
        "                                \n",
        "                              ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gbl3hU46AmOY"
      },
      "outputs": [],
      "source": [
        "class PointCloudData(Dataset):\n",
        "    def __init__(self, dataframe_path, valid=False, sample_rate=1024, transform=default_transforms()):\n",
        "        # Get data\n",
        "        self.df = pd.read_csv(dataframe_path)\n",
        "        # class dict\n",
        "        self.classes = {\"anomaly\": 1, \"normal\": 0}\n",
        "        self.sample_rate=sample_rate\n",
        "        self.transforms=transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __preproc__(self, file):\n",
        "      # Cloud is loaded\n",
        "        point_cloud = o3d.io.read_point_cloud(file)\n",
        "        # Cloud is downsampled to selected size\n",
        "        resampled = point_cloud.farthest_point_down_sample(num_samples=self.sample_rate)\n",
        "        points = resampled.points\n",
        "        np_array = np.array(points)\n",
        "        if self.transforms:\n",
        "            pointcloud = self.transforms(np_array)\n",
        "        return torch.from_numpy(pointcloud)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pcd_path = self.df.iloc[idx]['object_path']\n",
        "        category = self.df.iloc[idx]['label']\n",
        "        # with open(pcd_path, 'r') as f:\n",
        "        pointcloud = self.__preproc__(pcd_path)\n",
        "        return {'pointcloud': pointcloud, \n",
        "                'category': self.classes[category]}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA-1BelCQZ13"
      },
      "outputs": [],
      "source": [
        "# Point net parameters and data loaders\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "sample_rate = 1024\n",
        "epochs = 10\n",
        "\n",
        "train_ds = PointCloudData(\"./data/train_df.csv\", sample_rate=sample_rate)\n",
        "test_ds = PointCloudData(\"./data/test_df.csv\", sample_rate=sample_rate)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_ds, batch_size=batch_size)\n",
        "test_loader = DataLoader(dataset=test_ds, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIa7zMa-SQwh",
        "outputId": "b435a7b6-8627-462c-9ecc-bffbb78b40ee"
      },
      "outputs": [],
      "source": [
        "# Initialize PointNet\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "pointnet = PointNet()\n",
        "pointnet.to(device)\n",
        "optimizer = torch.optim.Adam(pointnet.parameters(), lr=learning_rate)\n",
        "\n",
        "pointnet.load_state_dict(torch.load(\"./model_weights/PointNet_9.pth\")) # Load most succesful model\n",
        "pointnet.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vvALDcESgSM",
        "outputId": "38d872bb-92a0-4c9d-9f7b-7c41e1f15065"
      },
      "outputs": [],
      "source": [
        "# We extract level_1 (512 values) and level_2 (256 values) embeddings\n",
        "level_1_embs = []\n",
        "level_2_embs = []\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(train_loader):\n",
        "        print('Batch [%4d / %4d]' % (i+1, len(train_loader)))\n",
        "                   \n",
        "        pcd = data['pointcloud'].to(device).float()\n",
        "        labels = data['category'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        _, _, _, emb1, emb2 = pointnet(pcd.transpose(1,2))\n",
        "        # _, preds = torch.max(outputs.data, 1)\n",
        "        level_1_embs += list(emb1.cpu().numpy())\n",
        "        level_2_embs += list(emb2.cpu().numpy())\n",
        "\n",
        "lvl1_np = np.array(level_1_embs)\n",
        "lvl2_np = np.array(level_2_embs)\n",
        "\n",
        "# We do the same for the test dataset\n",
        "test_level_1_embs = []\n",
        "test_level_2_embs = []\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        print('Batch [%4d / %4d]' % (i+1, len(test_loader)))\n",
        "                   \n",
        "        pcd = data['pointcloud'].to(device).float()\n",
        "        labels = data['category'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        _, _, _, emb1, emb2 = pointnet(pcd.transpose(1,2))\n",
        "        # _, preds = torch.max(outputs.data, 1)\n",
        "        test_level_1_embs += list(emb1.cpu().numpy())\n",
        "        test_level_2_embs += list(emb2.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9l9m91dUDDT"
      },
      "outputs": [],
      "source": [
        "# We read aggregated data created in PointNet notebook\n",
        "train_anomaly_mesh = pd.read_csv(\"./data/aggregated_train.csv\")\n",
        "test_anomaly_mesh = pd.read_csv(\"./data/aggregated_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4SEYeM7m21k"
      },
      "outputs": [],
      "source": [
        "# We concatenate the embeddings to their relevant aggregated data.\n",
        "# The aggregated data was generated in the PointNet notebook\n",
        "full_train_dataset = []\n",
        "for aggr_file, np_emb in zip(train_anomaly_mesh.iterrows(), lvl1_np):\n",
        "  full_train_dataset.append(np.concatenate([aggr_file[1].drop([\"label_obj_is_anom\"]).values, np_emb]))\n",
        "\n",
        "full_train_dataset_lvl2 = []\n",
        "for aggr_file, np_emb in zip(train_anomaly_mesh.iterrows(), lvl2_np):\n",
        "  full_train_dataset_lvl2.append(np.concatenate([aggr_file[1].drop([\"label_obj_is_anom\"]).values, np_emb]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "1zb1JWtialrU",
        "outputId": "4ce9463f-2c13-49c4-8086-fe13c2557a67"
      },
      "outputs": [],
      "source": [
        "# We load data for labels\n",
        "train_df = pd.read_csv(\"./data/train_df.csv\")\n",
        "test_df = pd.read_csv(\"./data/test_df.csv\")\n",
        "\n",
        "rf_mesh = RandomForestClassifier()\n",
        "features = np.array(full_train_dataset)\n",
        "labels = np.array([1 if val == \"anomaly\" else 0 for val in train_df['label']])\n",
        "rf_mesh.fit(features, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raVNTvhjbLiv"
      },
      "outputs": [],
      "source": [
        "# Test dataset level_1 and level_2 embeddings\n",
        "\n",
        "full_test_dataset = []\n",
        "for aggr_file, np_emb in zip(test_anomaly_mesh.iterrows(), test_level_1_embs):\n",
        "  full_test_dataset.append(np.concatenate([aggr_file[1].drop([\"label_obj_is_anom\"]).values, np_emb]))\n",
        "\n",
        "full_test_dataset_lvl2 = []\n",
        "for aggr_file, np_emb in zip(test_anomaly_mesh.iterrows(), test_level_2_embs):\n",
        "  full_test_dataset_lvl2.append(np.concatenate([aggr_file[1].drop([\"label_obj_is_anom\"]).values, np_emb]))\n",
        "\n",
        "preds = rf_mesh.predict(full_test_dataset)\n",
        "test_labels = np.array([1 if val == \"anomaly\" else 0 for val in test_df['label']])\n",
        "cm = confusion_matrix(test_labels, preds);\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8xWDAK_bRqa",
        "outputId": "e87a58be-4684-41c6-c55d-6e548e4b9f89"
      },
      "outputs": [],
      "source": [
        "lvl2_features = np.array(full_train_dataset_lvl2)\n",
        "labels = np.array([1 if val == \"anomaly\" else 0 for val in train_df['label']])\n",
        "rf_mesh_lvl2 = RandomForestClassifier()\n",
        "rf_mesh_lvl2.fit(lvl2_features, labels)\n",
        "preds_lvl2 = rf_mesh_lvl2.predict(full_test_dataset_lvl2)\n",
        "cm = confusion_matrix(test_labels, preds_lvl2);\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk45e9eDsBBO",
        "outputId": "8ca28275-04c1-4856-84b0-295a275f48ee"
      },
      "outputs": [],
      "source": [
        "# We save the model\n",
        "dump(rf_mesh, \"./model_weights/rf_mesh+PointNet_emb_512.joblib\")\n",
        "dump(rf_mesh_lvl2, \"./model_weights/rf_mesh+PointNet_emb_256.joblib\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_9nrR1ZvNFtI"
      },
      "source": [
        "### 90-10 split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MFfOGgS8p90"
      },
      "outputs": [],
      "source": [
        "def get_inds(df_90_10, full_df):\n",
        "  inds = []\n",
        "  for idx, filename in enumerate(full_df[\"object_path\"]):\n",
        "    if filename in df_90_10[\"object_path\"].tolist():\n",
        "      inds.append(idx)\n",
        "  return inds\n",
        "\n",
        "test_anomaly_mesh_90_10 = test_anomaly_mesh.iloc[get_inds(pd.read_csv(\"./data/test_df_90_10.csv\"), test_df)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kOL51zzNrq5",
        "outputId": "5b688523-07d5-4543-ecc6-0e071049d1e4"
      },
      "outputs": [],
      "source": [
        "test_dataset_90_10_lvl1 = []\n",
        "test_dataset_90_10_lvl2 = []\n",
        "for i in range(test_anomaly_mesh_90_10.shape[0]):\n",
        "  idx = test_anomaly_mesh_90_10.index[i]\n",
        "  test_dataset_90_10_lvl1.append(np.concatenate([test_anomaly_mesh_90_10.iloc[i].drop([\"label_obj_is_anom\"]).values, \n",
        "                                                 test_level_1_embs[idx]]))\n",
        "  test_dataset_90_10_lvl2.append(np.concatenate([test_anomaly_mesh_90_10.iloc[i].drop([\"label_obj_is_anom\"]).values, \n",
        "                                                 test_level_2_embs[idx]]))\n",
        "\n",
        "test_labels_90_10 = test_anomaly_mesh_90_10[\"label_obj_is_anom\"].astype(int)\n",
        "test_dataset_90_10_lvl1 = np.array(test_dataset_90_10_lvl1)\n",
        "test_dataset_90_10_lvl2 = np.array(test_dataset_90_10_lvl2)\n",
        "\n",
        "preds_90_10 = rf_mesh.predict(test_dataset_90_10_lvl1)\n",
        "cm_90_10 = confusion_matrix(test_labels_90_10, preds_90_10);\n",
        "print(\"90/10 split embedding 512\")\n",
        "print(cm_90_10)  \n",
        "\n",
        "preds_lvl2_90_10 = rf_mesh_lvl2.predict(test_dataset_90_10_lvl2)\n",
        "cm_lvl2_90_10 = confusion_matrix(test_labels_90_10, preds_lvl2_90_10);\n",
        "print(\"90/10 split embedding 256\")\n",
        "print(cm_lvl2_90_10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bxh5zwnoKIS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
