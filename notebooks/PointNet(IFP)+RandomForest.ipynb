{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvFMI7rv-iwn",
        "outputId": "72763919-b70a-4127-c6be-41746ffd6c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "# !cp \"/content/drive/MyDrive/ma-vqc_datasets/Quartered anomaly size.zip\" \"/content\"\n",
        "# # Restart runtime before running rest of code\n",
        "# !pip install open3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "--NMQPXB-x69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
            "[Open3D INFO] WebRTC GUI backend enabled.\n",
            "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import open3d as o3d\n",
        "import zipfile\n",
        "import glob\n",
        "from joblib import load, dump\n",
        "import pathlib\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import scipy.spatial.distance\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torch import nn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "random.seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UoMfh3J-1Ha"
      },
      "outputs": [],
      "source": [
        "# zip_name = \"Quartered anomaly size\"\n",
        "\n",
        "# # Load random forest model model for set anomaly size\n",
        "# !cp \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/RandomForestModels/rf_point_50-50.joblib\" \"/content/\"\n",
        "# # Load best performing PointNet model\n",
        "# !cp \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/PointNetModels/PointNet (IFP)/PointNet_90-10_sample_rate_1024_batch_size_32_lr_0.0001_9.pth\" \"/content/\"\n",
        "\n",
        "\n",
        "\n",
        "# with zipfile.ZipFile(f\"/content/{zip_name}.zip\", 'r') as zip_ref: \n",
        "#     zip_ref.extractall(\"/content/\")\n",
        "\n",
        "\n",
        "# # Preprocessing step to change paths to Google Colab compatible paths\n",
        "\n",
        "# # Correcting paths to be Google Colab compatible\n",
        "# def preprocess_path(path, dataset_name):\n",
        "#   idx = path.rindex(dataset_name) \n",
        "#   replaced = path[idx:].replace(\"\\\\\", \"/\") \n",
        "#   return os.path.join(\"/content/\", replaced)\n",
        "\n",
        "\n",
        "# # Concatenate all dataset versions\n",
        "# train_df = pd.concat([pd.read_csv(f\"/content/{zip_name}/Var1/point_cloud_mesh/corrected_train_50-50_class_ration.csv\"),\n",
        "#                 pd.read_csv(f\"/content/{zip_name}/Var2/point_cloud_mesh/corrected_train_50-50_class_ration.csv\"),\n",
        "#                 pd.read_csv(f\"/content/{zip_name}/Var3/point_cloud_mesh/corrected_train_50-50_class_ration.csv\"),\n",
        "#                 pd.read_csv(f\"/content/{zip_name}/Var4/point_cloud_mesh/corrected_train_50-50_class_ration.csv\")])\n",
        "\n",
        "# test_df = pd.concat([pd.read_csv(f\"/content/{zip_name}/Var1/point_cloud_mesh/corrected_test_50-50_class_ration.csv\"),\n",
        "#                 pd.read_csv(f\"/content/{zip_name}/Var2/point_cloud_mesh/corrected_test_50-50_class_ration.csv\"),\n",
        "#                 pd.read_csv(f\"/content/{zip_name}/Var3/point_cloud_mesh/corrected_test_50-50_class_ration.csv\"),\n",
        "#                 pd.read_csv(f\"/content/{zip_name}/Var4/point_cloud_mesh/corrected_test_50-50_class_ration.csv\")])\n",
        "# print(train_df.shape)\n",
        "# print(test_df.shape)\n",
        "\n",
        "\n",
        "# train_df[\"object_pcd_path\"] = [preprocess_path(path, zip_name) for path in train_df[\"object_pcd_path\"].values]\n",
        "# train_df[\"object_path\"] = [preprocess_path(path, zip_name) for path in train_df[\"object_path\"].values]\n",
        "# train_df[\"reference_path\"] = [preprocess_path(path, zip_name) for path in train_df[\"reference_path\"].values]\n",
        "\n",
        "# test_df[\"object_pcd_path\"] = [preprocess_path(path, zip_name) for path in test_df[\"object_pcd_path\"].values]\n",
        "# test_df[\"object_path\"] = [preprocess_path(path, zip_name) for path in test_df[\"object_path\"].values]\n",
        "# test_df[\"reference_path\"] = [preprocess_path(path, zip_name) for path in test_df[\"reference_path\"].values]\n",
        "\n",
        "\n",
        "# train_df.to_csv(\"/content/train_df.csv\", index=False)\n",
        "# test_df.to_csv(\"/content/test_df.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPFpBe-SPVrT"
      },
      "outputs": [],
      "source": [
        "# Copy random forest model\n",
        "# Load best performing PointNets\n",
        "# !cp \"/content/drive/MyDrive/MA-VQC Data/Halved Anomaly Size/PointNetModels/PointNet (RF)/PointNet_90-10_RF_samples_1024_batch_size_32_lr_0.0001_9.pth\" \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0jFLocDPQj1P"
      },
      "outputs": [],
      "source": [
        "# Load point model\n",
        "rf_point = load(\"./model_weights/rf_point.joblib\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ieBe7Nrq-6lc"
      },
      "source": [
        "## PointNet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "izCIumko-51a"
      },
      "outputs": [],
      "source": [
        "# Used for extracting feature rich vector giving a 1 dimensional vector for point net\n",
        "\n",
        "class Tnet(nn.Module):\n",
        "   def __init__(self, k=3):\n",
        "      super().__init__()\n",
        "      self.k=k\n",
        "      self.conv1 = nn.Conv1d(k,64,1)\n",
        "      self.conv2 = nn.Conv1d(64,128,1)\n",
        "      self.conv3 = nn.Conv1d(128,1024,1)\n",
        "      self.fc1 = nn.Linear(1024,512)\n",
        "      self.fc2 = nn.Linear(512,256)\n",
        "      self.fc3 = nn.Linear(256,k*k)\n",
        "\n",
        "      self.bn1 = nn.BatchNorm1d(64)\n",
        "      self.bn2 = nn.BatchNorm1d(128)\n",
        "      self.bn3 = nn.BatchNorm1d(1024)\n",
        "      self.bn4 = nn.BatchNorm1d(512)\n",
        "      self.bn5 = nn.BatchNorm1d(256)\n",
        "       \n",
        "\n",
        "   def forward(self, input):\n",
        "      # input.shape == (bs,n,3)\n",
        "      bs = input.size(0)\n",
        "      xb = F.relu(self.bn1(self.conv1(input)))\n",
        "      xb = F.relu(self.bn2(self.conv2(xb)))\n",
        "      xb = F.relu(self.bn3(self.conv3(xb)))\n",
        "      pool = nn.MaxPool1d(xb.size(-1))(xb)\n",
        "      flat = nn.Flatten(1)(pool)\n",
        "      xb = F.relu(self.bn4(self.fc1(flat)))\n",
        "      xb = F.relu(self.bn5(self.fc2(xb)))\n",
        "      \n",
        "      #initialize as identity\n",
        "      init = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)\n",
        "      if xb.is_cuda:\n",
        "        init=init.cuda()\n",
        "      matrix = self.fc3(xb).view(-1,self.k,self.k) + init\n",
        "      return matrix\n",
        "\n",
        "# Used for position estimation and point estimation using global and\n",
        "# local coordinates\n",
        "class Transform(nn.Module):\n",
        "   def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input_transform = Tnet(k=3)\n",
        "        self.feature_transform = Tnet(k=64)\n",
        "        self.conv1 = nn.Conv1d(3,64,1)#(3,64,1)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(64,128,1)\n",
        "        self.conv3 = nn.Conv1d(128,1024,1)\n",
        "       \n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "       \n",
        "   def forward(self, input):\n",
        "        matrix3x3 = self.input_transform(input)\n",
        "        # batch matrix multiplication\n",
        "        xb = torch.bmm(torch.transpose(input,1,2), matrix3x3).transpose(1,2)\n",
        "\n",
        "        xb = F.relu(self.bn1(self.conv1(xb)))\n",
        "\n",
        "        matrix64x64 = self.feature_transform(xb)\n",
        "        xb = torch.bmm(torch.transpose(xb,1,2), matrix64x64).transpose(1,2)\n",
        "\n",
        "        xb = F.relu(self.bn2(self.conv2(xb)))\n",
        "        xb = self.bn3(self.conv3(xb))\n",
        "        xb = nn.MaxPool1d(xb.size(-1))(xb)\n",
        "        output = nn.Flatten(1)(xb)\n",
        "        return output, matrix3x3, matrix64x64\n",
        "\n",
        "class PointNet(nn.Module):\n",
        "    def __init__(self, classes = 2):\n",
        "        super().__init__()\n",
        "        self.transform = Transform()\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, classes)\n",
        "        \n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        xb, matrix3x3, matrix64x64 = self.transform(input)\n",
        "        xb1 = F.relu(self.bn1(self.fc1(xb)))\n",
        "        xb2 = F.relu(self.bn2(self.dropout(self.fc2(xb1))))\n",
        "        output = self.fc3(xb2)\n",
        "        return self.logsoftmax(output), matrix3x3, matrix64x64, xb1, xb2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G4Euhnug_BYg"
      },
      "outputs": [],
      "source": [
        "# PointNet custom loss calculating with all 3 model outputs\n",
        "\n",
        "# LOSS\n",
        "def pointnetloss(outputs, labels, m3x3, m64x64, alpha = 0.0001):\n",
        "    criterion = torch.nn.NLLLoss()\n",
        "    bs = outputs.size(0)\n",
        "    id3x3 = torch.eye(3, requires_grad=True).repeat(bs, 1, 1)\n",
        "    id64x64 = torch.eye(64, requires_grad=True).repeat(bs, 1, 1)\n",
        "    if outputs.is_cuda:\n",
        "        id3x3 = id3x3.cuda()\n",
        "        id64x64 = id64x64.cuda()\n",
        "    diff3x3 = id3x3 - torch.bmm(m3x3, m3x3.transpose(1, 2))\n",
        "    diff64x64 = id64x64 - torch.bmm(m64x64, m64x64.transpose(1, 2))\n",
        "    return criterion(outputs, labels) + alpha * (torch.norm(diff3x3) + torch.norm(diff64x64)) / float(bs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xK0YvUKA_fOZ"
      },
      "source": [
        "Data loader for PointNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1SmotiDL76MK"
      },
      "outputs": [],
      "source": [
        "# Normalization used as a preprocessing step\n",
        "class Normalize(object):\n",
        "    def __call__(self, pointcloud):\n",
        "        assert len(pointcloud.shape)==2\n",
        "        \n",
        "        norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0) \n",
        "        norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))\n",
        "\n",
        "        return  norm_pointcloud\n",
        "\n",
        "def default_transforms():\n",
        "    return transforms.Compose([\n",
        "                                Normalize(),\n",
        "                                \n",
        "                              ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Gbl3hU46AmOY"
      },
      "outputs": [],
      "source": [
        "class PointCloudData(Dataset):\n",
        "    def __init__(self, dataframe_path, valid=False, sample_rate=1024, transform=default_transforms()):\n",
        "        # Get data\n",
        "        self.df = pd.read_csv(dataframe_path)\n",
        "        # class dict\n",
        "        self.classes = {\"anomaly\": 1, \"normal\": 0}\n",
        "        self.sample_rate=sample_rate\n",
        "        self.transforms=transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __preproc__(self, file):\n",
        "      # Cloud is loaded\n",
        "        point_cloud = o3d.io.read_point_cloud(file)\n",
        "        # Cloud is downsampled to selected size\n",
        "        resampled = point_cloud.farthest_point_down_sample(num_samples=self.sample_rate)\n",
        "        points = resampled.points\n",
        "        np_array = np.array(points)\n",
        "        if self.transforms:\n",
        "            pointcloud = self.transforms(np_array)\n",
        "        return torch.from_numpy(pointcloud)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pcd_path = self.df.iloc[idx]['object_path']\n",
        "        category = self.df.iloc[idx]['label']\n",
        "        # with open(pcd_path, 'r') as f:\n",
        "        pointcloud = self.__preproc__(pcd_path)\n",
        "        return {'pointcloud': pointcloud, \n",
        "                'category': self.classes[category]}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YA-1BelCQZ13"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "sample_rate = 1024\n",
        "epochs = 10\n",
        "\n",
        "train_ds = PointCloudData(\"./data/train_df.csv\", sample_rate=sample_rate)\n",
        "test_ds = PointCloudData(\"./data/test_df.csv\", sample_rate=sample_rate)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_ds, batch_size=batch_size)\n",
        "test_loader = DataLoader(dataset=test_ds, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIa7zMa-SQwh",
        "outputId": "b435a7b6-8627-462c-9ecc-bffbb78b40ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "pointnet = PointNet()\n",
        "pointnet.to(device)\n",
        "optimizer = torch.optim.Adam(pointnet.parameters(), lr=learning_rate)\n",
        "\n",
        "pointnet.load_state_dict(torch.load(\"./model_weights/PointNet_9.pth\"))\n",
        "pointnet.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vvALDcESgSM",
        "outputId": "38d872bb-92a0-4c9d-9f7b-7c41e1f15065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch [   1 /    1]\n"
          ]
        }
      ],
      "source": [
        "level_1_embs = []\n",
        "level_2_embs = []\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(train_loader):\n",
        "        print('Batch [%4d / %4d]' % (i+1, len(train_loader)))\n",
        "                   \n",
        "        pcd = data['pointcloud'].to(device).float()\n",
        "        labels = data['category'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        _, _, _, emb1, emb2 = pointnet(pcd.transpose(1,2))\n",
        "        # _, preds = torch.max(outputs.data, 1)\n",
        "        level_1_embs += list(emb1.cpu().numpy())\n",
        "        level_2_embs += list(emb2.cpu().numpy())\n",
        "\n",
        "lvl1_np = np.array(level_1_embs)\n",
        "lvl2_np = np.array(level_2_embs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xtCEENS9W_-l"
      },
      "outputs": [],
      "source": [
        "# Create aggregated data\n",
        "def percentile(n):\n",
        "    def percentile_(x):\n",
        "        return np.percentile(x, n)\n",
        "    percentile_.__name__ = 'percentile_%s' % n\n",
        "    return percentile_\n",
        "\n",
        "def is_anom(series):\n",
        "  return series.max()\n",
        "\n",
        "\n",
        "#Aggregate data for training the model to detect anomaly mesh\n",
        "def aggregate_data(df):\n",
        "  aggr_df = df.groupby(['id']).agg(\n",
        "      {'label_obj': is_anom,#\"first\",\n",
        "      'dist':[\n",
        "          'mean',\n",
        "          'max',\n",
        "          percentile(60),\n",
        "          percentile(70),\n",
        "          percentile(80),\n",
        "          percentile(90),\n",
        "          percentile(95),\n",
        "          percentile(96),\n",
        "          percentile(97),\n",
        "          percentile(98),\n",
        "          percentile(99),\n",
        "      ],\n",
        "      'prob_anom':[\n",
        "          'mean',  \n",
        "          'max', \n",
        "          percentile(60),\n",
        "          percentile(70),\n",
        "          percentile(80),\n",
        "          percentile(90),\n",
        "          percentile(95),\n",
        "          percentile(96),\n",
        "          percentile(97),\n",
        "          percentile(98),\n",
        "          percentile(99),\n",
        "      ],\n",
        "      }\n",
        "  )\n",
        "  aggr_df.columns = ['_'.join(col) for col in aggr_df.columns.values]\n",
        "  return aggr_df\n",
        "\n",
        "def create_rf_point_samples(df, rf_point, sample_rate, name):\n",
        "  full_aggr_df = None\n",
        "  new_path_list = []\n",
        "  for idx, (path, label) in enumerate(zip(df[\"reference_path\"], df[\"label\"])):\n",
        "    tmp_df = pd.read_csv(path)\n",
        "    tmp_df[\"id\"] = idx\n",
        "    tmp_df[\"label_obj\"] = 1 if label == \"anomaly\" else 0\n",
        "    preds = rf_point.predict_proba(tmp_df.loc[:, [\"x\", \"y\", \"z\", \"dist\"]].values)[:, 1]\n",
        "    tmp_df[\"prob_anom\"] = preds\n",
        "\n",
        "    aggr = aggregate_data(tmp_df)\n",
        "    if isinstance(full_aggr_df, type(None)):\n",
        "      full_aggr_df = aggr\n",
        "    else:\n",
        "      full_aggr_df = pd.concat([full_aggr_df,\n",
        "                              aggr], ignore_index=True)\n",
        "      \n",
        "  return full_aggr_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_gKNhcz74pH"
      },
      "outputs": [],
      "source": [
        "# !cp \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/RandomForestData/aggregated_train.zip\" \"/content/\"\n",
        "# !cp \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/RandomForestData/aggregated_test.zip\" \"/content/\"\n",
        "\n",
        "\n",
        "# with zipfile.ZipFile(f\"/content/aggregated_train.zip\", 'r') as zip_ref: \n",
        "#     zip_ref.extractall(\"\")\n",
        "\n",
        "# with zipfile.ZipFile(f\"/content/aggregated_test.zip\", 'r') as zip_ref: \n",
        "#     zip_ref.extractall(\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cAga-TnRDvf"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# test_anomaly_mesh = create_rf_point_samples(test_df, rf_point, sample_rate, \"test\")\n",
        "# train_anomaly_mesh = create_rf_point_samples(train_df, rf_point, sample_rate, \"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-9l9m91dUDDT"
      },
      "outputs": [],
      "source": [
        "train_anomaly_mesh = pd.read_csv(\"./data/aggregated_train.csv\")\n",
        "test_anomaly_mesh = pd.read_csv(\"./data/aggregated_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Q4SEYeM7m21k"
      },
      "outputs": [],
      "source": [
        "full_train_dataset = []\n",
        "for aggr_file, np_emb in zip(train_anomaly_mesh.iterrows(), lvl1_np):\n",
        "  full_train_dataset.append(np.concatenate([aggr_file[1].drop([\"label_obj_is_anom\"]).values, np_emb]))\n",
        "\n",
        "full_train_dataset_lvl2 = []\n",
        "for aggr_file, np_emb in zip(train_anomaly_mesh.iterrows(), lvl2_np):\n",
        "  full_train_dataset_lvl2.append(np.concatenate([aggr_file[1].drop([\"label_obj_is_anom\"]).values, np_emb]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "1zb1JWtialrU",
        "outputId": "4ce9463f-2c13-49c4-8086-fe13c2557a67"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "RandomForestClassifier()"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"./data/train_df.csv\")\n",
        "test_df = pd.read_csv(\"./data/test_df.csv\")\n",
        "\n",
        "rf_mesh = RandomForestClassifier()\n",
        "features = np.array(full_train_dataset)\n",
        "labels = np.array([1 if val == \"anomaly\" else 0 for val in train_df['label']])\n",
        "rf_mesh.fit(features, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RudyfNEQbTaZ",
        "outputId": "12628606-f86e-487a-80cf-c618c6df5a98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch [   1 /    1]\n"
          ]
        }
      ],
      "source": [
        "test_level_1_embs = []\n",
        "test_level_2_embs = []\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        print('Batch [%4d / %4d]' % (i+1, len(test_loader)))\n",
        "                   \n",
        "        pcd = data['pointcloud'].to(device).float()\n",
        "        labels = data['category'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        _, _, _, emb1, emb2 = pointnet(pcd.transpose(1,2))\n",
        "        # _, preds = torch.max(outputs.data, 1)\n",
        "        test_level_1_embs += list(emb1.cpu().numpy())\n",
        "        test_level_2_embs += list(emb2.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "raVNTvhjbLiv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[2 2]\n",
            " [1 3]]\n"
          ]
        }
      ],
      "source": [
        "full_test_dataset = []\n",
        "for aggr_file, np_emb in zip(test_anomaly_mesh.iterrows(), test_level_1_embs):\n",
        "  full_test_dataset.append(np.concatenate([aggr_file[1].drop([\"label_obj_is_anom\"]).values, np_emb]))\n",
        "\n",
        "full_test_dataset_lvl2 = []\n",
        "for aggr_file, np_emb in zip(test_anomaly_mesh.iterrows(), test_level_2_embs):\n",
        "  full_test_dataset_lvl2.append(np.concatenate([aggr_file[1].drop([\"label_obj_is_anom\"]).values, np_emb]))\n",
        "\n",
        "preds = rf_mesh.predict(full_test_dataset)\n",
        "test_labels = np.array([1 if val == \"anomaly\" else 0 for val in test_df['label']])\n",
        "cm = confusion_matrix(test_labels, preds);\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8xWDAK_bRqa",
        "outputId": "e87a58be-4684-41c6-c55d-6e548e4b9f89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[2 2]\n",
            " [4 0]]\n"
          ]
        }
      ],
      "source": [
        "lvl2_features = np.array(full_train_dataset_lvl2)\n",
        "labels = np.array([1 if val == \"anomaly\" else 0 for val in train_df['label']])\n",
        "rf_mesh_lvl2 = RandomForestClassifier()\n",
        "rf_mesh_lvl2.fit(lvl2_features, labels)\n",
        "preds_lvl2 = rf_mesh_lvl2.predict(full_test_dataset_lvl2)\n",
        "cm = confusion_matrix(test_labels, preds_lvl2);\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk45e9eDsBBO",
        "outputId": "8ca28275-04c1-4856-84b0-295a275f48ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['./model_weights/rf_mesh+PointNet_emb_256.joblib']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from joblib import dump\n",
        "dump(rf_mesh, \"./model_weights/rf_mesh+PointNet_emb_512.joblib\")\n",
        "dump(rf_mesh_lvl2, \"./model_weights/rf_mesh+PointNet_emb_256.joblib\")\n",
        "# !cp \"/content/rf_mesh+PointNet_emb_512_50-50.joblib\" \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/RandomForestModels\"\n",
        "# !cp \"/content/rf_mesh+PointNet_emb_256_50-50.joblib\" \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/RandomForestModels\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_9nrR1ZvNFtI"
      },
      "source": [
        "90-10 split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RyQxScwNFQI"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/MyDrive/MA-VQC Data/Quartered Anomaly Size/RandomForestData/test_df_90_10.csv\" \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_MFfOGgS8p90"
      },
      "outputs": [],
      "source": [
        "def get_inds(df_90_10, full_df):\n",
        "  inds = []\n",
        "  for idx, filename in enumerate(full_df[\"object_path\"]):\n",
        "    if filename in df_90_10[\"object_path\"].tolist():\n",
        "      inds.append(idx)\n",
        "  return inds\n",
        "\n",
        "test_anomaly_mesh_90_10 = test_anomaly_mesh.iloc[get_inds(pd.read_csv(\"./data/test_df_90_10.csv\"), test_df)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kOL51zzNrq5",
        "outputId": "5b688523-07d5-4543-ecc6-0e071049d1e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "90/10 split embedding 512\n",
            "[[2 2]\n",
            " [0 0]]\n",
            "90/10 split embedding 256\n",
            "[[2 2]\n",
            " [0 0]]\n"
          ]
        }
      ],
      "source": [
        "# test_point_df_90_10[\"prob_anom\"] = rf_point.predict_proba(test_point_df_90_10.loc[:, ['x', 'y', 'z', 'dist']].values)[:,1]\n",
        "# test_anomaly_mesh_90_10 = aggregate_data(test_point_df_90_10)\n",
        "\n",
        "test_dataset_90_10_lvl1 = []\n",
        "test_dataset_90_10_lvl2 = []\n",
        "for i in range(test_anomaly_mesh_90_10.shape[0]):\n",
        "  idx = test_anomaly_mesh_90_10.index[i]\n",
        "  test_dataset_90_10_lvl1.append(np.concatenate([test_anomaly_mesh_90_10.iloc[i].drop([\"label_obj_is_anom\"]).values, \n",
        "                                                 test_level_1_embs[idx]]))\n",
        "  test_dataset_90_10_lvl2.append(np.concatenate([test_anomaly_mesh_90_10.iloc[i].drop([\"label_obj_is_anom\"]).values, \n",
        "                                                 test_level_2_embs[idx]]))\n",
        "\n",
        "test_labels_90_10 = test_anomaly_mesh_90_10[\"label_obj_is_anom\"].astype(int)\n",
        "test_dataset_90_10_lvl1 = np.array(test_dataset_90_10_lvl1)\n",
        "test_dataset_90_10_lvl2 = np.array(test_dataset_90_10_lvl2)\n",
        "\n",
        "preds_90_10 = rf_mesh.predict(test_dataset_90_10_lvl1)\n",
        "cm_90_10 = confusion_matrix(test_labels_90_10, preds_90_10);\n",
        "print(\"90/10 split embedding 512\")\n",
        "print(cm_90_10)  \n",
        "\n",
        "preds_lvl2_90_10 = rf_mesh_lvl2.predict(test_dataset_90_10_lvl2)\n",
        "cm_lvl2_90_10 = confusion_matrix(test_labels_90_10, preds_lvl2_90_10);\n",
        "print(\"90/10 split embedding 256\")\n",
        "print(cm_lvl2_90_10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bxh5zwnoKIS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
